# 千帆


## 基于API应用开发模式

1. 【AI端】调用API接口（几行代码/Python语言），可通过Prompt设计引入相关知识（无需修改模型的结构或参数）
2. 【前端】写web端/移动端界面、交互（html/css/js/gradio/streamlit）
3. 【后端】整体任务逻辑/队列、集成和调用AI端能力、返回AI端结果

所需能力：无需有AI算法能力，需有前后端开发和工程落地能力

总结：AI部分难度较小，无需AI专业知识，主要开发工作在前后端和工程化落地。


## 基于大模型的微调应用开发模式

### Fine-tuning

1. 【AI端】加载已经预训练好的大模型，准备和上传训练数据（与目标任务相关的数据，符合fine-tuning文件格式），训练新的Fine-tuning模型，也可以加入一些分类层等；模型封装和部署：高性能/高并发/高可用
  a. 【数据】数据获取，仿照给定数据示例构建新的数据集（字段：Prompt-Completion）
2. 【前端】写web端/移动端界面、交互逻辑（html/css/js/gradio/streamlit）
3. 【后端】整体任务逻辑/队列、集成和调用AI端能力、返回AI端结果

所需能力：深度学习算法原理、跨模态数据处理、模型训练、模型部署/弹性部署；前后端开发和工程落地

总结：
1. AI部分难度较大，需懂深度学习原理，会数据处理（跨模态数据融合），会训练模型；算力资源消耗大。
2. 难点主要在于数据工程（数据采集、清洗、对齐）和训练推理资源。
3. 随着模型规模增大，效果提升逐渐饱和。

### Prompt-tuning

1. 【AI端】设计预训练语言模型的任务、设计输入模板样式(Prompt Engineering)、设计label 样式及模型的输出映射到label的方式(Answer Engineering)，prompt-completion-label；
  a. 【数据】标签数据准备、Verbalizer准备、prompt模板设定
2. 【前端】写web端/移动端界面、交互逻辑
3. 【后端】整体任务逻辑/队列、集成和调用AI端能力、返回AI端结果

所需能力：Prompt engineering相关技巧、前后端开发和工程落地

总结：无需数据标注，难点在于Prompt模板设计（人工设计模板/自动学习模板），需要根据下游任务和预训练模型的特性来选择合适的模板，微调消耗的存储和运算资源相比传统finetune有所降低。


## 基于大模型API或大模型微调+插件开发模式

**这种方式更适合我们。**

将垂直行业的领域知识向量化并存入向量数据库——用户提问——用户问题向量化——查询向量数据库，得到TopN条匹配知识——构建Prompt，调用 API——返回回答.

例如：向量知识库embedding

总结：AI部分难度较小，难点在于整体任务流程的工程化思考和落地。对任务流程拆解的工程化思考要求较高（节点/队列流程设计、弹性部署等）

## 相关链接 

[基于大模型的应用开发方式介绍](https://cloud.baidu.com/qianfandev/topic/267755)

[如何准备用于微调的数据集](https://cloud.baidu.com/qianfandev/topic/267759)

[基于文心大模型开发的应用在应用商店/微信小程序上架指南](https://cloud.baidu.com/qianfandev/topic/267218)



## 名词解释 

SFT：这是Selective Fine-tuning的缩写，被翻译为选择性精调。这是一种模型训练策略，在预训练模型的基础上，选择性地对部分参数进行精细调整，以适应特定的任务。这样可以在保留模型在大规模数据上训练得到的通用知识的同时，提升模型在特定任务上的性能。

Post-pretrain：也被称为后期预训练或者模型微调，通常是指在一个预训练模型（已在大量数据上进行过训练）的基础上，进行进一步的训练或优化，以适应特定任务或特定数据。

RLHF：这可能是指Reinforcement Learning with Human Feedback的缩写，即通过人类反馈进行强化学习的过程。在这一过程中，人类反馈被用作奖励信号，以指导模型进行学习和优化。


## SFT 

  * 数据导入
    * 导入数据集
  * 数据标注
    * 对导入数据集进行逐一标注、很费时费力啊（10w数据 需要20+的团队做一周, 后续的微调还需要专业的ai团队）
    * 标注完成后，需要发布，否则训练任务中无法找到数据集
  * 训练配置
    * 根据训练量花钱的，训练前面发布的数据集
  * 模型纳管
    * 前面训练完成的数据集，点击发布。就可以成为自己的模型了
  * 发布服务
    * 发线上服务可就花钱了
  * 体验测试


SFT是采用预先训练好的网络模型，针对我们的专门任务，在少量的监督数据上进行重新训练的技术。

需要选择 多轮对话、非排序类的数据集。

数据集要求标注的数据就是正确答案，要做到问题和回答类型多样化，场景多样，避免重复，尽可能覆盖全场景

数据集标注必须达到100%，才能进行发布。发布后才能被使用。

数据集中10%以上的数据长度超过4k, 就需要选择8k版本模型

训练模式：
  * 全量更新： 更新了基础模型的全部参数。需要数据量巨大。场景：样本数量较多，并且主要注重这些效果的推理能力，不考虑大模型通用能力。
  * LoRA: 更新了基础模型局部参数。场景：样本数量少于1000，并且需要保留大模型自身的通用能力。
  * prompt tuning: 保持了模型全部参数不变，作为提示词训练，使用提示词来引导模型更好适应特定的任务。能力最弱，不建议用。

迭代轮次，选择适合的轮次，太高会过拟合。**数据越多，轮次越少（数据量不够就需要更多次的训练）。100条数据15轮，1000条数据10轮，10000条数据2轮**

批处理大小，越大就越越能加速训练。但是可能导致内存问题。**千帆平台已经做了限制，选最大值就行。**


学习率，默认就行

**如何评估训练结果**

1. 模型只能回答训练过的内容，可能是过拟合，需要调整数据或超参数
2. 查看评估报告：loss 和 perplexity 的收敛曲线，成功的训练会有明显的收敛的过程以及收敛后趋于平稳的走势。**收敛出现在训练的后半部分**
  * loss 上升，是未收敛，需要增加数据或者训练轮次
  * loss 支持下降，是收敛未完成，需要增加数据或者训练轮次
  * perplexity 在训练1/3处接近1，同时loss接近0，是过拟合。需要把训练轮次减少到1/3重新训练
3. ROUGE 和 BLEU 评分。默认的基础模型大概只有不到20的评分。
  * 在确切答案的问题上，评分可达到100。例如：判断一道题是数学题还是语文题
  * 在开放式答案的问题上，评分达到50-60分已经很高了。例如：根据关键词写一个故事
  * 如果最终的评分还不如未训练的评分，则需要调整训练数据、超参配置等


## 知识库需要的流程

需要向量数据库

大模型挂载知识库的方法:

* As Context： 作为上下文引入，将知识直接加入到对话上下文
* Text Embeddings：将知识库向量化，存储到向量数据库。输入问题后，寻找相似分段，将相似分段和问题输入 LLM
* Fine tuning：微调训练，将知识库处理为微调数据集，对模型进行微调后使用

| 方法            | 知识大小                | 预处理成本 | 使用成本                                                               | 备注                                                     |
| --------------- | ----------------------- | ---------- | ---------------------------------------------------------------------- | -------------------------------------------------------- |
| 上下文引入      | 不能超过LLM的上下文限制 | 无         | 中\n每次均需要携带完整上下文                                           | 使用分段处理的方式能够提供较大文档的处理，但会损失细节。 |
| Text Embeddings | 不限制                  | 低         | 中\n携带上下文后，基本需要4000 tokens\ngpt-3.5-turbo 4k tokens =$0.008 | 选择的是相似度最高的TopK段落，存在理解不充分的问题。     |
| Fine tuning     | 不限制                  | 训练成本   | 高                                                                     | 微调后的模型需要单独部署。                               |



* 上传数据做知识库，然后创建对话插件选择知识库就可以


* 温度和多样性 二选一进行配置，选择温度的话，建议配置低一些。
* 可以选择其他插件集成、例如百度搜索
* 使用命中测试才测试知识库命中。不断的修改置信度，来修正。

### 问题

可以多个知识库么？  可以
知识库费用怎没算的？ 0.004元/千tokens


0. 可以千帆关闭或放弃润色，按标准答案输出？  **这么干就变成unit了**
1. 插件是否可以做流程引导？ 
2. 知识库可以调用接口么？ 我看到可以选择自定义插件，自定义插件是不是可以使用自己的接口？
3. 提示词怎么修改成自己的，调试信息里有一堆不是我写的东西。而且我自制的prompt模版在导入界面搜索不到 
4. 知识库答案中是否可以包含 markdown 或者 富文本 
5. 知识库回答速度较慢。是否有办法提升？
6. 我是否可以提供一些简单的问题来覆盖大模型的回答。例如：你是谁？ 回答我是掌上高考智能客服 
7. 根据教程中的案例，实际测试 提示词和其他插件都不生效 



## 数据集 和 知识库的区别 

数据集 需要大量的数据，百万级别以上。在AI工程师和数据标注的加持下，会更准、但是更贵， 纯算力250一天，训练价钱再根据训练数量计算。

知识库更方便、便宜
