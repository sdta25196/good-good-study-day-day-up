# 微调PPT


## 前置知识

什么是transformer

- transformer 架构通过特定的算法和预训练过程最终生成了一系列模型文件

![llama3模型文件](./assets/ft-llama3.jpg)


什么是微调


**什么是训练/预训练/微调/轻量化微调**

![training](./assets/ft-training.png)


模型训练（Training）

- 模型是一个算法，在现有的数据上使用这个算法就是训练。
- 训练的过程就是找到一组模型的参数，使模型在数据集上能够拟合的更好
- 训练的结果会得到一些模型文件

预训练（Pre-Training）

- 模型预训练（Pre-training）是在正式训练模型之前，使用大量数据对模型进行初步训练的过程。预训练的目的是让模型能够学习到一些通用的特征和知识，这样在后续针对特定任务的训练中，模型可以更快地收敛，并且往往能够获得更好的性能。

微调（Fine-Tuning）

- 微调（Fine-Tuning）是在模型预训练之后，使用特定任务的数据对模型进行进一步训练的过程

- 相较于未进行预训练的模型，在垂直领域训练所需的数据，大大减少

- 但是全量参数的微调。会需要巨大的算力

- 使用特定任务的数据集对预训练模型进行训练


轻量化微调（Parameter Efficient Fine-Tuning, PEFT）

![微调流程](./assets/ft-peft_process.png)


- 节约算力，在对预训练模型进行微调时，采用一些策略来减少模型的计算负担和存储需求，使其更适合在资源受限的环境中运行

- lora
- Qlora

流程：

- 定义微调数据集加载器
- 定义数据处理函数
- 加载预训练模型：AutoModel.from_pretrained(MODEL_NAME_OR_PATH)
- 在预训练模型上增加任务相关输出层 （如果需要）
- 加载预训练 Tokenizer：AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH)
- **定义注入参数的方法（见下文）**
- 定义各种超参
- 定义 Trainer
- 定义 Evaluation Metric
- 开始训练


方法:

- prefix-tuning(Ptv2)

在原本的transformer的前面，加上一个N层的隐层，训练时，只训练N层的参数。

- LoRA

目前最为常用的一个微调方法

在 Transformer 的参数矩阵上加一个低秩矩阵（A × B），只训练 A，B

![lora](./assets/ft-lora.png)


- QLoRA

![float](./assets/ft-float.png)

通常，权重参数以 32 位格式（FP32）存储，这意味着矩阵中的每个元素占用 32 位空间。 我们可以将相同的信息压缩为 8 位甚至 4 位。

它是指将模型权重从高精度数据类型转换为低精度数据类型的过程。



## 什么时候需要微调

知识类问题，尤其是垂直知识尽量选择RAG。微调是一件投入产出比非常小的事情。

微调更适合的是一些任务类的训练。

1. 我们无法清晰的使用prompt描述好这个任务，某种类型的文风训练。
2. 大模型本身能力不行，通过微调牺牲大模型本身的通用能力，来增强对某种任务类型的能力


## 怎么微调


- 准备数据集：
  收集和准备与您任务相关的数据集。数据集应该被划分为训练集、验证集和测试集。
  数据需要被预处理，包括清洗、分词、编码等步骤，以匹配GLM输入格式。
- 加载预训练模型：
  使用Hugging Face的Transformers库或其他相关工具，加载预训练的GLM模型以及相应的分词器（tokenizer）。
- 定义模型：
  在加载的GLM模型上添加必要的层，以适应您的特定任务。例如，对于分类任务，您可能需要在模型末尾添加一个全连接层。
  定义损失函数和优化器。对于分类任务通常使用交叉熵损失，对于回归任务可能使用均方误差等。
- 微调模型：
  使用训练集对模型进行训练。在训练过程中，您可能需要调整学习率、批量大小、训练轮数等超参数。
  在训练过程中，定期使用验证集来监控模型性能，并调整模型参数以避免过拟合。
- 评估模型：
  在微调完成后，使用测试集来评估模型的性能。
  根据评估结果，可能需要进行进一步的调整或数据增强。


## 实际微调的效果

生成条数任务，会非常明显的出现效果


## 微调QA

* 训练完成之后，换一种问法问不到怎么办？

训练数据中，没有构造这种问法，就没有训练到，模型就不会，就会出现幻觉。

* 为什么需要用多卡训练？

当模型太大的时候，一张卡装不下，就需要拆到多张卡上去训练。 如果一张卡能装下的话，一张卡比多张卡训练更快。

* RLHF的应用场景？

如果任务是封闭任务，就不需要RLHF，如果是比较开放的任务，就可以考虑用RLHF对模型再次训练。

例如：为了能够让大模型更加可控，包括价值观，输出结构等。 而高质量和人类价值观对齐的数据是非常庞大的，无法完全在fine-tuning中穷举，所以这部分可以让人类对输出的答案做一个选择，再次对模型进行fine-tuning。自然也就输出了符合人类价值观的数据。

* 利用模型大小判断所需GPU

模型参数 *4 是推理所需的显存。

目前模型的参数绝大多数都是float32类型, 占用4个字节。所以一个粗略的计算方法就是，每10亿个参数，占用4G显存(实际应该是10^9*4/1024/1024/1024=3.725G，为了方便可以记为4G)。


## agent、workflow




## 扩展

- 更详细的Transformer网络拆解（Encoder-Decoder）：https://jalammar.github.io/illustrated-transformer/

- 更详细的GPT模型拆解：https://jalammar.github.io/illustrated-gpt2/



