{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 选修内容\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神经网络的梯度怎么算\n",
    "\n",
    "Chain Rule:\n",
    "\n",
    "假设 $L(w)=f(g(h(w)))$\n",
    "\n",
    "那么 $L'(w)=f'(g(h(w))) \\cdot g'(h(w)) \\cdot h'(w)$\n",
    "\n",
    "<img src=\"backprop.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "蓝色的过程叫 Forward Pass，红色的过程叫 Backward Pass，整个过程叫 Backpropagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 几个常用的超参\n",
    "\n",
    "### 1、过拟合与欠拟合\n",
    "\n",
    "<br />\n",
    "<img src=\"overfit.png\" style=\"margin-left: 0px\" width=\"800px\">\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>奥卡姆剃刀：</b> 两个处于竞争地位的理论能得出同样的结论，那么简单的那个更好。\n",
    "</div>\n",
    "\n",
    "**防止过拟合的方法（1）：**Weight Decay\n",
    "\n",
    "$J(\\omega)=L(D,\\omega)+\\lambda\\|\\omega\\| \\Rightarrow \\nabla_{\\omega}J=\\nabla_{\\omega}L + \\frac{1}{2}\\lambda\\omega$\n",
    "\n",
    "- 惩罚参数的复杂性（$L_2$-norm）：等价与在梯度上减去参数本身（乘一个小数作为权重）\n",
    "- Weight Decay 就是前面那个权重$\\lambda$\n",
    "\n",
    "**防止过拟合的方法（2）：**Dropout\n",
    "\n",
    "- 我们在前向传播的时候，概率性的（临时）删除一部分神经元，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征\n",
    "- 这样训练$N$次，等价于训练$N$不同的网络，再取平均值；$N$个网络不会同时过拟合于与一个结果，这样平均值的方式能有效减少过拟合的干扰。\n",
    "\n",
    "<img src=\"dropout.jfif\" style=\"margin-left: 0px\" width=\"400px\">\n",
    "\n",
    "### 学习率调整策略\n",
    "\n",
    "- 开始时学习率大些：快速到达最优解附近\n",
    "- 逐渐减小学习率：避免跳过最优解\n",
    "- NLP 任务的损失函数有很多“悬崖峭壁”，自适应学习率更能处理这种极端情况，避免梯度爆炸。\n",
    "\n",
    "<img src=\"scheduler.png\" style=\"margin-left: 0px\" width=\"400px\">\n",
    "\n",
    "几种常用的学习率调整器\n",
    "\n",
    "<img src=\"lr_scheduler.jpg\" style=\"margin-left: 0px\" width=\"400px\">\n",
    "\n",
    "**防止过拟合的方法（3）：**学习率 Warm Up\n",
    "\n",
    "先从一个很小的学习率逐渐上升到正常学习率，在稳步减小学习率\n",
    "\n",
    "- 其原理尚未被充分证明\n",
    "- 经验主义解释：减缓模型在初始阶段对 mini-batch 的提前过拟合现象，保持分布的平稳\n",
    "- 经验主义解释：有助于保持模型深层的稳定性\n",
    "\n",
    "<img src=\"warmup.png\" style=\"margin-left: 0px\" width=\"400px\">\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>应用场景：</b> (1) 当网络非常容易nan时候；(2) 如果训练集损失很低，准确率高，但测试集损失大，准确率低. \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自然语言处理常见的网络结构\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>思考：</b> 图像天生可以表示成矩阵（或tensor），那文本怎么表示成矩阵（或tensor）\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、文本卷积神经网络 TextCNN\n",
    "\n",
    "<br />\n",
    "\n",
    "一个窗口的卷积和 Pooling 过程\n",
    "\n",
    "<img src=\"conv_maxpooling_steps.gif\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "不同大小的窗口分别做卷积和 Pooling，结果拼在一起\n",
    "\n",
    "<img src=\"TextCNN.jpg\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "- 参数量较少、好训练、算力要求低\n",
    "- 适合文本分类问题\n",
    "- 善于表示局部特征（卷积窗口），不擅长表示长上下文依赖关系\n",
    "\n",
    "### 2、循环神经网络 RNN\n",
    "\n",
    "<br />\n",
    "\n",
    "首先：输入是一个序列\n",
    "\n",
    "<img src=\"RNN.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "但这种简易 RNN 有很多问题，最大问题是随着序列长度增加，梯度消失或爆炸\n",
    "\n",
    "<img src=\"LSTM.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "LSTM 和 GRU 通过「门」来控制上文的状态被记住还是遗忘，同时防止梯度消失或爆炸\n",
    "\n",
    "以 LSTM 为例：\n",
    "\n",
    "<img src=\"lstm.jfif\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "### 3、Attention (for RNN)\n",
    "\n",
    "<br />\n",
    "\n",
    "给定当前的输入，上文的一些信息比另一些重要\n",
    "\n",
    "<img src=\"attention.gif\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "<br />\n",
    "\n",
    "<font color='blue'>于是设计一个可微的函数就可以把它加入到网络中来试试，反正也没有全局最优解</font>\n",
    "\n",
    "<br />\n",
    "<img src=\"attention-fn.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "- 对当前 token $t$的上文的每个 token $i$计算上述 score\n",
    "- 将这些 score 做 softmax 得到权重$\\alpha_{t,i}$\n",
    "- 将上文的隐层状态乘以其权重并相加$c_t=\\sum_i\\alpha_{t,i}h_i$\n",
    "- 将$c_t$与当前 token 的状态拼接在一起$s_t=\\mathrm{concat}(h_t,c_t)$\n",
    "- 激活输出$y_t=\\sigma(s_t)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer 江山一统\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>思考：</b> RNN有什么缺点？大模型为什么不是很多层RNN？\n",
    "</div>\n",
    "\n",
    "<br />\n",
    "<img src=\"transformer.gif\" style=\"margin-left: 0px\" width=\"800px\">\n",
    "\n",
    "### 1、**消除恐惧：**我们亲手写一个 Transformer\n",
    "\n",
    "#### 1.1、Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, max_len=512):\n",
    "        super().__init__()\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, embed_size).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, embed_size, 2).float()\n",
    "                    * -(math.log(10000.0) / embed_size)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class BERTEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param vocab_size: 词表大小\n",
    "        :param embed_size: embedding维度768\n",
    "        :param dropout: dropout概率\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(\n",
    "            vocab_size, embed_size, padding_idx=0)\n",
    "        self.position_embedding = PositionalEmbedding(\n",
    "            embed_size=embed_size, max_len=512)\n",
    "        self.token_type_embedding = nn.Embedding(2, embed_size, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids):\n",
    "        x = self.token_embedding(input_ids) + self.position_embedding(\n",
    "            input_ids) + self.token_type_embedding(token_type_ids)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2、单头 Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import math\n",
    "\n",
    "'''\n",
    "query = query_linear(x)\n",
    "key = key_linear(x)\n",
    "value = value_linear(x)\n",
    "'''\n",
    "\n",
    "# 单个头的注意力计算\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "\n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "            / math.sqrt(query.size(-1))\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "\n",
    "        return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "每个token对应的query向量与每个token对应的key向量做内积\n",
    "\n",
    "<img src=\"kq2.gif\" style=\"margin-left: 0px\" width=\"800px\">\n",
    "\n",
    "<br />\n",
    "将上述内积取softmax（得到0~1之间的值，即为attention权重）\n",
    "\n",
    "<img src=\"kq_softmax.gif\" style=\"margin-left: 0px\" width=\"800px\">\n",
    "\n",
    "<br />\n",
    "计算每个token相对于所有其它token的attention权重（最终构成一个$L\\times L$的attention矩阵）\n",
    "\n",
    "<img src=\"kq_softmax2.gif\" style=\"margin-left: 0px\" width=\"800px\">\n",
    "\n",
    "<br />\n",
    "每个token对应的value向量乘以attention权重，并相加，得到当前token的self-attention value向量\n",
    "\n",
    "<img src=\"v.gif\" style=\"margin-left: 0px\" width=\"800px\">\n",
    "\n",
    "<br />\n",
    "将上述操作应用于每个token\n",
    "<img src=\"v2.gif\" style=\"margin-left: 0px\" width=\"800px\">\n",
    "\n",
    "<br />\n",
    "以上是一个头的操作，同时（并行）应用于多个独立的头\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3、多头 Attention\n",
    "\n",
    "将每个头得到向量拼接在一起，最后乘一个线性矩阵，得到 multi-head attention 的输出\n",
    "\n",
    "<img src=\"multi-head.gif\" style=\"margin-left: 0px\" width=\"800px\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, head_num, hidden_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param head_num: 头的个数，必须能被hidden_size整除\n",
    "        :param hidden_size: 隐层的维度，与embed_size一致\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert hidden_size % head_num == 0\n",
    "\n",
    "        self.per_head_dim = hidden_size // head_num\n",
    "        self.head_num = head_num\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.query_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.key_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.value_linear = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.output_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.attention = Attention()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def reshape(self, x, batch_size):\n",
    "        # 拆成多个头\n",
    "        return x.view(batch_size, -1, self.head_num, self.per_head_dim).transpose(1, 2)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        query = self.reshape(self.query_linear(x))\n",
    "        key = self.reshape(self.key_linear(x))\n",
    "        value = self.reshape(self.value_linear(x))\n",
    "\n",
    "        # 每个头计算attention\n",
    "        x, attn = self.attention(\n",
    "            query, key, value, mask=mask, dropout=self.dropout\n",
    "        )\n",
    "\n",
    "        # 把每个头的attention*value拼接在一起\n",
    "        x = x.transpose(1, 2).contiguous().view(\n",
    "            batch_size, -1, self.hidden_size)\n",
    "\n",
    "        # 乘一个线性矩阵\n",
    "        return self.output_linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4、全连接网络（Feed-Forward Network）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.input_layer = nn.Linear(hidden_size, hidden_size*4)\n",
    "        self.output_layer = nn.Linear(hidden_size*4, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5、拼成一层 Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, head_num, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = MultiHeadedAttention(head_num, hidden_size)\n",
    "        self.feed_forward = FeedForward(hidden_size, dropout=dropout)\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x0 = x\n",
    "        # 多头注意力层\n",
    "        x = self.multi_head_attention(x, mask)\n",
    "\n",
    "        # 残差和LayerNorm层(1)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.layer_norm1(x0+x)\n",
    "\n",
    "        # 前向网络层\n",
    "        x1 = x\n",
    "        x = self.feed_forward(x)\n",
    "\n",
    "        # 残差和LayerNorm层(2)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.layer_norm2(x1+x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "Multi-head attention的输出，经过残差和norm之后进入一个两层全连接网络\n",
    "<img src=\"ffn.gif\" style=\"margin-left: 0px\" width=\"800px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layernorm:\n",
    "\n",
    "$y=\\frac{x-\\mathrm{E}(x)}{\\sqrt{\\mathrm{Var}(x)+\\epsilon}}*\\gamma+\\beta$\n",
    "\n",
    "其中 $\\gamma$ 和 $\\beta$ 是可训练的参数，$\\epsilon=10^{-5}$是超参，保持数值稳定性。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6、多层 Transformer 构成 BERT Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class BERT(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, hidden_size=768, layer_num=12, head_num=12, dropout=0.1):\n",
    "\n",
    "        super().__init__()\n",
    "        # Embedding层\n",
    "        self.embedding = BERTEmbedding(\n",
    "            vocab_size=vocab_size, embed_size=hidden_size)\n",
    "        # N层Transformers\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [TransformerBlock(hidden_size, head_num, dropout)\n",
    "             for _ in range(layer_num)]\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids):\n",
    "        \"\"\"\n",
    "        tokenizer([\"你好吗\",\"你好\"], text_pair=[\"我很好\",\"我好\"], max_length=10, padding='max_length',truncation=True)\n",
    "        [CLS]你好吗[SEP]我很好[SEP][PAD]\n",
    "        [CLS]你好[SEP]我好[SEP][PAD][PAD][PAD]  \n",
    "        input_ids: [\n",
    "            [101, 872, 1962, 1408, 102, 2769, 2523, 1962, 102, 0],\n",
    "            [101, 872, 1962, 102, 2769, 1962, 102, 0, 0, 0]\n",
    "        ]\n",
    "        token_type_ids：[\n",
    "                [0, 0, 0, 0, 0, 1, 1, 1, 1, 0],\n",
    "                [0, 0, 0, 0, 1, 1, 1, 0, 0, 0]\n",
    "            ]\n",
    "        \"\"\"\n",
    "        attention_mask = (x > 0).unsqueeze(\n",
    "            1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "\n",
    "        # 计算embedding\n",
    "        x = self.embedding(input_ids, token_type_ids)\n",
    "\n",
    "        # 逐层代入Tranformers\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer.forward(x, attention_mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、Transformer 怎么用\n",
    "\n",
    "#### 2.1、 Encoder-Only LM 用于文本表示\n",
    "\n",
    "针对不同下游任务，在 Encoder 上面添加不同的输出层\n",
    "<br />\n",
    "<img src=\"BERT.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "#### 2.2、 Encoder上加头，面向不同任务\n",
    "\n",
    "#### Linear Head文本分类\n",
    "\n",
    "<img src=\"bert-classification.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "#### BERT-BiLSTM-CRF一个序列标注的经典网络结构\n",
    "\n",
    "<img src=\"bert-bilstm-crf.jpg\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "#### 2.3、 Encoder-Decoder LM，机器翻译/文本生成（大语言模型的一种形态）\n",
    "\n",
    "- Decoder 也是 N 层 transformer 结构\n",
    "- 生成一个 token，把它加入上文，再生成下一个 token，以此类推\n",
    "  <br />\n",
    "  <img src=\"decoder1.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "1. Decoder 的每个 token 与 decoder 上文的 token 做一次 attention\n",
    "\n",
    "2. 输出 add & norm 之后再与 encoder 最后一层的输出做一次 sttention\n",
    "\n",
    "<br />\n",
    "<img src=\"decoder2.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>注意：</b> Decoder 的 token 只能 attend 到上文的 token（因为此时下文还没有出现）\n",
    "</div>\n",
    "\n",
    "<br />\n",
    "<img src=\"decoder3.png\" style=\"margin-left: 0px\" width=\"600px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4、Decoder-Only LM 也叫 Causal LM 或 Left-to-right LM（GPT 家族）\n",
    "\n",
    "<br />\n",
    "<img src=\"decoder4.png\" style=\"margin-left: 0px\" width=\"400px\">\n",
    "\n",
    "#### 2.5、 大语言模型族谱\n",
    "\n",
    "<br />\n",
    "<img src=\"llm.jpg\" style=\"margin-left: 0px\" width=\"800px\">\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
