{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad8a1ee4-2da9-4cda-867f-ac10f58d4cb0",
   "metadata": {},
   "source": [
    "## 1、数据准备与处理\n",
    "\n",
    "1. 需要准备什么数据，取决于你希望模型完成哪些功能\n",
    "2. 数据尽可能覆盖各种可能出现的情况\n",
    "\n",
    "例如：你要模型识别地址，训练数据就要尽可能覆盖地址的各种表达形式和去一个地点的各种说法\n",
    "   - <font color='blue'>我要去</font><font color='red'>天安门</font>\n",
    "   - <font color='red'>太古大厦A座</font><font color='blue'>怎么走</font>\n",
    "   - <font color='blue'>导航到</font><font color='red'>东方东路19号</font><font color='green'>途经</font><font color='red'>Joy Cafe</font>\n",
    "\n",
    "### 数据采集\n",
    "\n",
    "- 自然来源（如业务日志）：真实数据\n",
    "- Web 抓取：近似数据\n",
    "- 人造\n",
    "\n",
    "### 数据标注\n",
    "\n",
    "- 专业标注公司\n",
    "  - 定标准，定验收指标\n",
    "  - 预标注\n",
    "  - 反馈与优化\n",
    "  - 正式标注\n",
    "  - 抽样检查：合格->验收；不合格->返工\n",
    "- 众包\n",
    "  - 定标准，定检验指标\n",
    "  - 抽样每个工作者的质量\n",
    "  - 维系高质量标注者社区\n",
    "- 主动学习：通过模型选择重要样本，由专家标注，再训练模型\n",
    "- 设计产品形态，在用户自然交互中产生标注数据（例如点赞、收藏）\n",
    "\n",
    "### 数据清洗\n",
    "\n",
    "- 去除不相关数据\n",
    "- 去除冗余数据（例如重复的样本）\n",
    "- 去除误导性数据（业务相关）\n",
    "\n",
    "### 样本均衡性\n",
    "\n",
    "- 尽量保证每个标签（场景/子问题）都有足够多的训练样本\n",
    "- 每个标签对应的数据量尽量相当\n",
    "  - 或者在保证每个标签样本充值的前提下，数据分布尽量接近真实业务场景的数据分布\n",
    "- 数据不均衡时的策略\n",
    "  - 数据增强：为数据不够类别造数据：（1）人工造；（2）通过模板生成再人工标注；（3）由模型自动生成（再人工标注/筛选）\n",
    "  - 数据少的类别数据绝对数量也充足时，Downsample 一般比 Upsample 效果好\n",
    "  - 实在没办法的话，在训练 loss 里加权（一般不是最有效的办法）\n",
    "- 根据业务属性，保证其他关键要素的数据覆盖，例如：时间因素、地域因素、用户年龄段等\n",
    "\n",
    "### 数据集构建\n",
    "\n",
    "- 数据充分的情况下\n",
    "  - 切分训练集（训练模型）、验证集（验证超参）、测试集（检验最终模型+超参的效果）\n",
    "  - 以随机采样的方式保证三个集合的数据分布一致性\n",
    "  - 在以上三个集合里都尽量保证各个类别/场景的数据覆盖\n",
    "- 数据实在太少\n",
    "  - 交叉验证\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52357236-8a65-4bb4-91c7-55f2aac951f1",
   "metadata": {},
   "source": [
    "## 2、LLM 输出时每个 Token 的概率是怎么得到的\n",
    "\n",
    "<img src=\"lmhead.png\" width=800px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c73530e-eac6-4a6e-b767-a052e71bd373",
   "metadata": {},
   "source": [
    "## 3、多卡训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5452c456-25fb-48be-be54-0ea65d7614fa",
   "metadata": {},
   "source": [
    "### 启动脚本\n",
    "\n",
    "单机多卡\n",
    "\n",
    "```sh\n",
    "torchrun\n",
    "    --standalone\n",
    "    --nnodes=1\n",
    "    --nproc-per-node=$NUM_OF_GPUS\n",
    "    train.py (--args ...)\n",
    "```\n",
    "\n",
    "多机多卡\n",
    "\n",
    "```sh\n",
    "torchrun\n",
    "    --nnodes=$NUM_NODES\n",
    "    --nproc-per-node=$NUM_OF_GPUS_PER_NODE\n",
    "    --max-restarts=3\n",
    "    --rdzv-id=$JOB_ID\n",
    "    --rdzv-backend=c10d\n",
    "    --rdzv-endpoint=$HOST_NODE_ADDR\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1f76e2-bf9a-443a-8021-c731aac17356",
   "metadata": {},
   "source": [
    "代码样例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a2024c-2e38-43ff-9ec3-ae1ccaec902d",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch\n",
    "import torch.distributed\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from transformers import GPT2TokenizerFast, DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import multiprocessing\n",
    "import random\n",
    "\n",
    "MODEL_NAME = \"gpt2\"\n",
    "SEED = 42\n",
    "MAX_LENGTH = 1024\n",
    "MIN_LENGTH = 4\n",
    "VOCAB_SIZE = 50257\n",
    "\n",
    "# 定义数据处理函数\n",
    "def prepare_train_features(examples):\n",
    "    # 略过空行\n",
    "    examples[\"nonempty_text\"] = [\n",
    "        d.strip() for d in examples[\"text\"] if len(d.strip()) > 0\n",
    "    ]\n",
    "\n",
    "    # Convert the tokens into ids using the trained tokenizer\n",
    "\n",
    "    tokenized_example = tokenizer(\n",
    "        examples[\"nonempty_text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH*100,\n",
    "    )\n",
    "\n",
    "    # 模型输出的字段\n",
    "    examples[\"input_ids\"] = []\n",
    "    examples[\"attention_mask\"] = []\n",
    "\n",
    "    del examples[\"text\"]\n",
    "    del examples[\"nonempty_text\"]\n",
    "\n",
    "    for input_ids, attention_mask in zip(tokenized_example[\"input_ids\"], tokenized_example[\"attention_mask\"]):\n",
    "\n",
    "        trunc_ids = input_ids[:min(len(input_ids), MAX_LENGTH)]\n",
    "        trunc_mask = attention_mask[:min(len(attention_mask), MAX_LENGTH)]\n",
    "\n",
    "        # 把长句切成MAX_LENGTH长度的片段, 最后一段如果小于MIN_LENGTH则忽略\n",
    "        while len(trunc_ids) > MIN_LENGTH:\n",
    "            trunc_len = len(trunc_ids)\n",
    "            if trunc_len < MAX_LENGTH:\n",
    "                examples[\"input_ids\"].append(\n",
    "                    trunc_ids+[tokenizer.pad_token_id]*(MAX_LENGTH-trunc_len))\n",
    "                examples[\"attention_mask\"].append(\n",
    "                    trunc_mask+[0]*(MAX_LENGTH-trunc_len))\n",
    "            else:\n",
    "                examples[\"input_ids\"].append(trunc_ids)\n",
    "                examples[\"attention_mask\"].append(trunc_mask)\n",
    "\n",
    "            input_ids = input_ids[trunc_len:]\n",
    "            attention_mask = attention_mask[trunc_len:]\n",
    "\n",
    "            trunc_ids = input_ids[:min(len(input_ids), MAX_LENGTH)]\n",
    "            trunc_mask = attention_mask[:min(len(attention_mask), MAX_LENGTH)]\n",
    "\n",
    "    examples['labels'] = examples['input_ids'].copy()\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "# 开启多开训练模式\n",
    "# 现在pytorch已经把这部封装了，不用手工写了\n",
    "# torch.distributed.init_process_group(\n",
    "#    backend='nccl', init_method=\"env://\", rank=args.local_rank, world_size=args.word_size)\n",
    "# torch.cuda.set_device(args.local_rank)\n",
    "\n",
    "# 自动下载openwebtext数据集，展开前几十GB，展开成arrow格式大约500G\n",
    "raw_datasets = load_dataset(\"openwebtext\", split=\"train\")\n",
    "# 这里只用1%的数据作为测试集，否则每次dev时间很长\n",
    "raw_datasets = raw_datasets.train_test_split(test_size=0.01)\n",
    "\n",
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_valid_dataset = raw_datasets[\"test\"]\n",
    "\n",
    "transformers.set_seed(args.seed)\n",
    "\n",
    "# 定义tokenizer\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(MODEL_NAME)\n",
    "# 获取CPU核数（用于数据加载线程数）\n",
    "num_proc = multiprocessing.cpu_count()\n",
    "\n",
    "if torch.distributed.get_rank() > 0:\n",
    "    # 主进程加载数据，其它进程等待从缓存加载arrow文件\"\n",
    "    torch.distributed.barrier()\n",
    "\n",
    "tokenized_train_dataset = raw_train_dataset.map(\n",
    "    prepare_train_features,\n",
    "    batched=True,\n",
    "    num_proc=num_proc\n",
    ")\n",
    "\n",
    "tokenized_valid_dataset = raw_valid_dataset.map(\n",
    "    prepare_train_features,\n",
    "    batched=True,\n",
    "    num_proc=num_proc\n",
    ")\n",
    "\n",
    "if torch.distributed.get_rank() == 0:\n",
    "    # 主进程加载数据结束\n",
    "    torch.distributed.barrier()\n",
    "\n",
    "# 定义数据校准器（自动生成batch）\n",
    "collater = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# 定义GPT-2模型config\n",
    "model_config = GPT2Config(vocab_size=VOCAB_SIZE,\n",
    "                          max_position_embeddings=MAX_LENGTH, return_dict=True)\n",
    "# 定义模型（此处参数随机初始化）\n",
    "model = GPT2LMHeadModel(config=model_config)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./my_model\",        # checkpoint保存路径\n",
    "    evaluation_strategy=\"steps\",    # 每N步做一次eval\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,             # 训练epoch数\n",
    "    per_device_train_batch_size=8,  # 每张卡的batch大小\n",
    "    gradient_accumulation_steps=20,   # 累加几个step做一次参数更新\n",
    "    per_device_eval_batch_size=16,  # evaluation batch size\n",
    "    logging_steps=1000,             # 每1000步eval一次\n",
    "    save_steps=1000,                # 每1000步保存一个checkpoint\n",
    "    learning_rate=1e-3,             # 学习率\n",
    "    warmup_steps=2000,              # 预热（可选）\n",
    "    optim=\"adamw_hf\",               # 求解器（默认）\n",
    ")\n",
    "\n",
    "# 定义训练器\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collater,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_valid_dataset,\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()\n",
    "\n",
    "if trainer.is_world_process_zero():\n",
    "    # 避免每个进程保存一次\n",
    "    trainer.save_model()\n",
    "    \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dfac1c-20e7-4c77-b44c-dc319d3b1ad8",
   "metadata": {},
   "source": [
    "## 4、GPU 利用率\n",
    "\n",
    "### 常见的造成 GPU 利用率低的原因\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93239b42-50f4-4ffd-91b5-fb5bf24a9989",
   "metadata": {},
   "source": [
    "本质是**CPU 的计算**或**I/O**的环节耗时长，导致 GPU 利用率上不去\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589e2f65-49ba-4c01-8988-f32675f952df",
   "metadata": {},
   "source": [
    "### 数据加载与处理的耗时\n",
    "\n",
    "Dataloader 的几个相关参数\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9178480e-a6b6-441d-976b-51dc273af922",
   "metadata": {},
   "source": [
    "DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=None,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, *, prefetch_factor=2,\n",
    "           persistent_workers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9476a2-46b6-442e-b302-4e7a1bbf8166",
   "metadata": {},
   "source": [
    "- num_workers：线程数\n",
    "- prefetch_factor：每个 worker 提前加载样本数，设置不合理可能会让 CPU 处理时 GPU 空闲\n",
    "- pin_memory：直接将数据映射到 GPU 的相关内存块上，省掉一点数据传输时间\n",
    "\n",
    "另外，数据处理函数逻辑太复杂也影响资源利用率，建议减少 for/while 循环。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d0857b-333b-4bc4-bcd2-f65b64303a39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 减少 I/O 操作的耗时\n",
    "\n",
    "- 模型保存不宜太频繁(--save_steps 参数)\n",
    "- 日志打印、指标上报、进度上报等不宜太频繁\n",
    "- 存储介质对时延影响也很明显\n",
    "  - 本地存储介质性能：SSD > ceph > cfs-1.5 > hdfs > mdfs\n",
    "  - 网络存储：数据与计算最好在同城；排查路由、网络带宽等其它因素。\n",
    "- 数据不宜分成太多小文件，会影响 I/O 性能（主要是图像处理场景）\n",
    "- 分布式训练时要使用 DistributedDataParallel （Pytorch）\n",
    "- 多机训练要启用 GDRDMA （英伟达的远程直接显存访问机制）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1f0e2f-4e4f-4888-9844-9aa7562c94df",
   "metadata": {},
   "source": [
    "### 其它 CPU 计算耗时\n",
    "\n",
    "主要是 loss 计算和 metric 计算的复杂度（常见的问题不涉及此处，主要是自定义 loss 或 metric 需要注意这个问题）\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
