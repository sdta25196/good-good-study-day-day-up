{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¡ è¿™èŠ‚è¯¾ä¼šå¸¦ç»™ä½ \n",
    "\n",
    "1. å¦‚ä½•ä½¿ç”¨ LangChainï¼šä¸€å¥—åœ¨å¤§æ¨¡å‹èƒ½åŠ›ä¸Šå°è£…çš„å·¥å…·æ¡†æ¶\n",
    "2. å¦‚ä½•ç”¨å‡ è¡Œä»£ç å®ç°ä¸€ä¸ªå¤æ‚çš„ AI åº”ç”¨\n",
    "3. é¢å‘å¤§æ¨¡å‹çš„æµç¨‹å¼€å‘çš„è¿‡ç¨‹æŠ½è±¡\n",
    "\n",
    "å¼€å§‹ä¸Šè¯¾ï¼\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å†™åœ¨å‰é¢\n",
    "\n",
    "- LangChain ä¹Ÿæ˜¯ä¸€å¥—é¢å‘å¤§æ¨¡å‹çš„å¼€å‘æ¡†æ¶ï¼ˆSDKï¼‰\n",
    "- LangChain æ˜¯ AGI æ—¶ä»£è½¯ä»¶å·¥ç¨‹çš„ä¸€ä¸ªæ¢ç´¢å’ŒåŸå‹\n",
    "- LangChain è¿­ä»£é€Ÿåº¦æ˜æ˜¾å¿«äº Semantic Kernelï¼Œå‡ ä¹æ˜å¤©ä¸€ä¸ªç‰ˆæœ¬\n",
    "- å­¦ä¹  Langchain è¦å…³æ³¨æ¥å£å˜æ›´\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain vs. Semantic Kernel\n",
    "\n",
    "[![Star History Chart](https://api.star-history.com/svg?repos=langchain-ai/langchain,microsoft/semantic-kernel,langchain-ai/langchainjs&type=Date)](https://star-history.com/#langchain-ai/langchain&microsoft/semantic-kernel&langchain-ai/langchainjs&Date)\n",
    "\n",
    "æ•°æ®æ¥æºï¼šhttps://star-history.com/#langchain-ai/langchain&microsoft/semantic-kernel&langchain-ai/langchainjs&Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## LangChain çš„æ ¸å¿ƒç»„ä»¶\n",
    "\n",
    "1. æ¨¡å‹ I/O å°è£…\n",
    "   - LLMsï¼šå¤§è¯­è¨€æ¨¡å‹\n",
    "   - Chat Modelsï¼šä¸€èˆ¬åŸºäº LLMsï¼Œä½†æŒ‰å¯¹è¯ç»“æ„é‡æ–°å°è£…\n",
    "   - PromptTempleï¼šæç¤ºè¯æ¨¡æ¿\n",
    "   - OutputParserï¼šè§£æè¾“å‡º\n",
    "2. æ•°æ®è¿æ¥å°è£…\n",
    "   - Document Loadersï¼šå„ç§æ ¼å¼æ–‡ä»¶çš„åŠ è½½å™¨\n",
    "   - Document Transformersï¼šå¯¹æ–‡æ¡£çš„å¸¸ç”¨æ“ä½œï¼Œå¦‚ï¼šsplit, filter, translate, extract metadata, etc\n",
    "   - Text Embedding Modelsï¼šæ–‡æœ¬å‘é‡åŒ–è¡¨ç¤ºï¼Œç”¨äºæ£€ç´¢ç­‰æ“ä½œï¼ˆå•¥æ„æ€ï¼Ÿåˆ«æ€¥ï¼Œåé¢è¯¦ç»†è®²ï¼‰\n",
    "   - Verctorstores: ï¼ˆé¢å‘æ£€ç´¢çš„ï¼‰å‘é‡çš„å­˜å‚¨\n",
    "   - Retrievers: å‘é‡çš„æ£€ç´¢\n",
    "3. è®°å¿†å°è£…\n",
    "   - Memoryï¼šè¿™é‡Œä¸æ˜¯ç‰©ç†å†…å­˜ï¼Œä»æ–‡æœ¬çš„è§’åº¦ï¼Œå¯ä»¥ç†è§£ä¸ºâ€œä¸Šæ–‡â€ã€â€œå†å²è®°å½•â€æˆ–è€…è¯´â€œè®°å¿†åŠ›â€çš„ç®¡ç†\n",
    "4. æ¶æ„å°è£…\n",
    "   - Chainï¼šå®ç°ä¸€ä¸ªåŠŸèƒ½æˆ–è€…ä¸€ç³»åˆ—é¡ºåºåŠŸèƒ½ç»„åˆ\n",
    "   - Agentï¼šæ ¹æ®ç”¨æˆ·è¾“å…¥ï¼Œè‡ªåŠ¨è§„åˆ’æ‰§è¡Œæ­¥éª¤ï¼Œè‡ªåŠ¨é€‰æ‹©æ¯æ­¥éœ€è¦çš„å·¥å…·ï¼Œæœ€ç»ˆå®Œæˆç”¨æˆ·æŒ‡å®šçš„åŠŸèƒ½\n",
    "     - Toolsï¼šè°ƒç”¨å¤–éƒ¨åŠŸèƒ½çš„å‡½æ•°ï¼Œä¾‹å¦‚ï¼šè°ƒ google æœç´¢ã€æ–‡ä»¶ I/Oã€Linux Shell ç­‰ç­‰\n",
    "     - Toolkitsï¼šæ“ä½œæŸè½¯ä»¶çš„ä¸€ç»„å·¥å…·é›†ï¼Œä¾‹å¦‚ï¼šæ“ä½œ DBã€æ“ä½œ Gmail ç­‰ç­‰\n",
    "5. Callbacks\n",
    "\n",
    "<img src=\"langchain.png\" style=\"margin-left: 0px\" width=500px>\n",
    "\n",
    "å®˜æ–¹æ–‡æ¡£åœ°å€ï¼šhttps://python.langchain.com/docs/get_started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸€ã€æ¨¡å‹ I/O å°è£…\n",
    "\n",
    "æŠŠä¸åŒçš„æ¨¡å‹ï¼Œç»Ÿä¸€å°è£…æˆä¸€ä¸ªæ¥å£ï¼Œæ–¹ä¾¿æ›´æ¢æ¨¡å‹è€Œä¸ç”¨é‡æ„ä»£ç ã€‚\n",
    "\n",
    "### 1.1 æ¨¡å‹ APIï¼šLLM vs. ChatModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å®‰è£…æœ€æ–°ç‰ˆæœ¬\n",
    "!pip install langchain==0.1.0\n",
    "!pip install langchain-openai # v0.1.0æ–°å¢çš„åº•åŒ…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 OpenAI æ¨¡å‹å°è£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆ‘æ˜¯OpenAIçš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚æˆ‘è¢«è®¾è®¡å‡ºæ¥æ˜¯ä¸ºäº†å¸®åŠ©è§£ç­”é—®é¢˜ã€æä¾›ä¿¡æ¯å’Œå¸®åŠ©ç”¨æˆ·å®Œæˆå„ç§ä»»åŠ¡ã€‚\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    " \n",
    "llm = ChatOpenAI(model=\"gpt-4\") # é»˜è®¤æ˜¯gpt-3.5-turbo\n",
    "response = llm.invoke(\"ä½ æ˜¯è°\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 å¤šè½®å¯¹è¯ Session å°è£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='æ‚¨æ˜¯å­¦å‘˜ç‹å“ç„¶ã€‚')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage, #ç­‰ä»·äºOpenAIæ¥å£ä¸­çš„assistant role\n",
    "    HumanMessage, #ç­‰ä»·äºOpenAIæ¥å£ä¸­çš„user role\n",
    "    SystemMessage #ç­‰ä»·äºOpenAIæ¥å£ä¸­çš„system role\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"ä½ æ˜¯AGIClassçš„è¯¾ç¨‹åŠ©ç†ã€‚\"), \n",
    "    HumanMessage(content=\"æˆ‘æ˜¯å­¦å‘˜ï¼Œæˆ‘å«ç‹å“ç„¶ã€‚\"), \n",
    "    AIMessage(content=\"æ¬¢è¿ï¼\"),\n",
    "    HumanMessage(content=\"æˆ‘æ˜¯è°\") \n",
    "]\n",
    "llm.invoke(messages) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>åˆ’é‡ç‚¹ï¼š</b>é€šè¿‡æ¨¡å‹å°è£…ï¼Œå®ç°ä¸åŒæ¨¡å‹çš„ç»Ÿä¸€æ¥å£è°ƒç”¨\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='æ‚¨å¥½ï¼Œæˆ‘æ˜¯ç™¾åº¦ç ”å‘çš„çŸ¥è¯†å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ï¼Œä¸­æ–‡åæ˜¯æ–‡å¿ƒä¸€è¨€ï¼Œè‹±æ–‡åæ˜¯ERNIE Botã€‚æˆ‘èƒ½å¤Ÿä¸äººå¯¹è¯äº’åŠ¨ï¼Œå›ç­”é—®é¢˜ï¼ŒååŠ©åˆ›ä½œï¼Œé«˜æ•ˆä¾¿æ·åœ°å¸®åŠ©äººä»¬è·å–ä¿¡æ¯ã€çŸ¥è¯†å’Œçµæ„Ÿã€‚\\n\\nå¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ã€‚')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# å…¶å®ƒæ¨¡å‹åˆ†è£…åœ¨ langchain_community åº•åŒ…ä¸­\n",
    "from langchain_community.chat_models import ErnieBotChat\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "ernie = ErnieBotChat()\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(content=\"ä½ æ˜¯è°\") \n",
    "]\n",
    "\n",
    "ernie.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 æ¨¡å‹çš„è¾“å…¥ä¸è¾“å‡º\n",
    "\n",
    "<img src=\"model_io.jpg\" style=\"margin-left: 0px\" width=500px>\n",
    "\n",
    "### 1.2.1 Promptæ¨¡æ¿å°è£…\n",
    "\n",
    "PromptTemplate å¯ä»¥åœ¨æ¨¡æ¿ä¸­è‡ªå®šä¹‰å˜é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['subject'] template='ç»™æˆ‘è®²ä¸ªå…³äº{subject}çš„ç¬‘è¯'\n",
      "ç»™æˆ‘è®²ä¸ªå…³äºå°æ˜çš„ç¬‘è¯\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"ç»™æˆ‘è®²ä¸ªå…³äº{subject}çš„ç¬‘è¯\")\n",
    "print(template)\n",
    "print(template.format(subject='å°æ˜'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='æˆ‘æ˜¯AGIè¯¾å ‚çš„å®¢æœåŠ©æ‰‹ï¼Œåå­—å«ç“œç“œã€‚æˆ‘å¯ä»¥å›ç­”å…³äºAGIè¯¾å ‚çš„é—®é¢˜ï¼Œæä¾›å¸®åŠ©å’Œæ”¯æŒã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts.chat import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\"ä½ æ˜¯{product}çš„å®¢æœåŠ©æ‰‹ã€‚ä½ çš„åå­—å«{name}\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "prompt = template.format_messages(\n",
    "        product=\"AGIè¯¾å ‚\",\n",
    "        name=\"ç“œç“œ\",\n",
    "        query=\"ä½ æ˜¯è°\"\n",
    "    )\n",
    "\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>åˆ’é‡ç‚¹ï¼š</b>æŠŠPromptæ¨¡æ¿çœ‹ä½œå¸¦æœ‰å‚æ•°çš„å‡½æ•°ï¼Œå¯ç±»æ¯”äº SK çš„ Semantic Function\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2ã€ä»æ–‡ä»¶åŠ è½½Promptæ¨¡æ¿"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yamlæ ¼å¼"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " _type: prompt\n",
    "input_variables:\n",
    "    [\"adjective\", \"content\"]\n",
    "template: \n",
    "    Tell me a {adjective} joke about {content}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSONæ ¼å¼"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{\n",
    "    \"_type\": \"prompt\",\n",
    "    \"input_variables\": [\"adjective\", \"content\"],\n",
    "    \"template\": \"Tell me a {adjective} joke about {content}.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Templateå•ç‹¬å­˜æ”¾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "cat simple_template.txt\n",
    "```\n",
    "\n",
    "```\n",
    "Tell me a {adjective} joke about {content}.\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{\n",
    "    \"_type\": \"prompt\",\n",
    "    \"input_variables\": [\"adjective\", \"content\"],\n",
    "    \"template_path\": \"simple_template.txt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åŠ è½½æ–¹å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a funny joke about Xiao Ming.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "\n",
    "prompt = load_prompt(\"simple_prompt.yaml\")\n",
    "\n",
    "# OR \n",
    "# prompt = load_prompt(\"simple_prompt.json\")\n",
    "\n",
    "print(prompt.format(adjective=\"funny\", content=\"Xiao Ming\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 è¾“å‡ºå°è£… OutputParser\n",
    "\n",
    "è‡ªåŠ¨æŠŠ LLM è¾“å‡ºçš„å­—ç¬¦ä¸²æŒ‰æŒ‡å®šæ ¼å¼åŠ è½½ã€‚\n",
    "\n",
    "LangChain å†…ç½®çš„ OutputParser åŒ…æ‹¬:\n",
    "\n",
    "- ListParser\n",
    "- DatetimeParser\n",
    "- EnumParser\n",
    "- PydanticParser\n",
    "- XMLParser\n",
    "\n",
    "ç­‰ç­‰\n",
    "\n",
    "### 1.3.1 Pydantic (JSON) Parser\n",
    "\n",
    "è‡ªåŠ¨æ ¹æ®Pydanticç±»çš„å®šä¹‰ï¼Œç”Ÿæˆè¾“å‡ºçš„æ ¼å¼è¯´æ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1  import BaseModel, Field, validator\n",
    "from typing import List, Dict\n",
    "\n",
    "# å®šä¹‰ä½ çš„è¾“å‡ºå¯¹è±¡\n",
    "class Date(BaseModel):\n",
    "    year: int = Field(description=\"Year\")\n",
    "    month: int = Field(description=\"Month\")\n",
    "    day: int = Field(description=\"Day\")\n",
    "    era: str = Field(description=\"BC or AD\")\n",
    "\n",
    "    # ----- å¯é€‰æœºåˆ¶ --------\n",
    "    # ä½ å¯ä»¥æ·»åŠ è‡ªå®šä¹‰çš„æ ¡éªŒæœºåˆ¶\n",
    "    @validator('month')\n",
    "    def valid_month(cls, field):\n",
    "        if field <= 0 or field > 12:\n",
    "            raise ValueError(\"æœˆä»½å¿…é¡»åœ¨1-12ä¹‹é—´\")\n",
    "        return field\n",
    "        \n",
    "    @validator('day')\n",
    "    def valid_day(cls, field):\n",
    "        if field <= 0 or field > 31:\n",
    "            raise ValueError(\"æ—¥æœŸå¿…é¡»åœ¨1-31æ—¥ä¹‹é—´\")\n",
    "        return field\n",
    "\n",
    "    @validator('day', pre=True, always=True)\n",
    "    def valid_date(cls, day, values):\n",
    "        year = values.get('year')\n",
    "        month = values.get('month')\n",
    "\n",
    "        # ç¡®ä¿å¹´ä»½å’Œæœˆä»½éƒ½å·²ç»æä¾›\n",
    "        if year is None or month is None:\n",
    "            return day  # æ— æ³•éªŒè¯æ—¥æœŸï¼Œå› ä¸ºæ²¡æœ‰å¹´ä»½å’Œæœˆä»½\n",
    "\n",
    "        # æ£€æŸ¥æ—¥æœŸæ˜¯å¦æœ‰æ•ˆ\n",
    "        if month == 2:\n",
    "            if cls.is_leap_year(year) and day > 29:\n",
    "                raise ValueError(\"é—°å¹´2æœˆæœ€å¤šæœ‰29å¤©\")\n",
    "            elif not cls.is_leap_year(year) and day > 28:\n",
    "                raise ValueError(\"éé—°å¹´2æœˆæœ€å¤šæœ‰28å¤©\")\n",
    "        elif month in [4, 6, 9, 11] and day > 30:\n",
    "            raise ValueError(f\"{month}æœˆæœ€å¤šæœ‰30å¤©\")\n",
    "\n",
    "        return day\n",
    "\n",
    "    @staticmethod\n",
    "    def is_leap_year(year):\n",
    "        if year % 400 == 0 or (year % 4 == 0 and year % 100 != 0):\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Format Instruction=====\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"year\": {\"title\": \"Year\", \"description\": \"Year\", \"type\": \"integer\"}, \"month\": {\"title\": \"Month\", \"description\": \"Month\", \"type\": \"integer\"}, \"day\": {\"title\": \"Day\", \"description\": \"Day\", \"type\": \"integer\"}, \"era\": {\"title\": \"Era\", \"description\": \"BC or AD\", \"type\": \"string\"}}, \"required\": [\"year\", \"month\", \"day\", \"era\"]}\n",
      "```\n",
      "====Prompt=====\n",
      "æå–ç”¨æˆ·è¾“å…¥ä¸­çš„æ—¥æœŸã€‚\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"year\": {\"title\": \"Year\", \"description\": \"Year\", \"type\": \"integer\"}, \"month\": {\"title\": \"Month\", \"description\": \"Month\", \"type\": \"integer\"}, \"day\": {\"title\": \"Day\", \"description\": \"Day\", \"type\": \"integer\"}, \"era\": {\"title\": \"Era\", \"description\": \"BC or AD\", \"type\": \"string\"}}, \"required\": [\"year\", \"month\", \"day\", \"era\"]}\n",
      "```\n",
      "ç”¨æˆ·è¾“å…¥:\n",
      "2023å¹´å››æœˆ6æ—¥å¤©æ°”æ™´...\n",
      "====æ¨¡å‹åŸå§‹è¾“å‡º=====\n",
      "content='{\"year\": 2023, \"month\": 4, \"day\": 6, \"era\": \"AD\"}'\n",
      "====Parseåçš„è¾“å‡º=====\n",
      "year=2023 month=4 day=6 era='AD'\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "\n",
    "model_name = 'gpt-4'\n",
    "temperature = 0\n",
    "model = ChatOpenAI(model_name=model_name, temperature=temperature)\n",
    "\n",
    "# æ ¹æ®Pydanticå¯¹è±¡çš„å®šä¹‰ï¼Œæ„é€ ä¸€ä¸ªOutputParser\n",
    "parser = PydanticOutputParser(pydantic_object=Date)\n",
    "\n",
    "template = \"\"\"æå–ç”¨æˆ·è¾“å…¥ä¸­çš„æ—¥æœŸã€‚\n",
    "{format_instructions}\n",
    "ç”¨æˆ·è¾“å…¥:\n",
    "{query}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"query\"],\n",
    "    # ç›´æ¥ä»OutputParserä¸­è·å–è¾“å‡ºæè¿°ï¼Œå¹¶å¯¹æ¨¡æ¿çš„å˜é‡é¢„å…ˆèµ‹å€¼\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()} \n",
    ")\n",
    "\n",
    "print(\"====Format Instruction=====\")\n",
    "print(parser.get_format_instructions())\n",
    "\n",
    "\n",
    "query = \"2023å¹´å››æœˆ6æ—¥å¤©æ°”æ™´...\"\n",
    "model_input = prompt.format_prompt(query=query)\n",
    "\n",
    "print(\"====Prompt=====\")\n",
    "print(model_input.to_string())\n",
    "\n",
    "output = model(model_input.to_messages())\n",
    "print(\"====æ¨¡å‹åŸå§‹è¾“å‡º=====\")\n",
    "print(output)\n",
    "print(\"====Parseåçš„è¾“å‡º=====\")\n",
    "date = parser.parse(output.content)\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Auto-Fixing Parser\n",
    "\n",
    "åˆ©ç”¨LLMè‡ªåŠ¨æ ¹æ®è§£æå¼‚å¸¸ä¿®å¤å¹¶é‡æ–°è§£æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===æ ¼å¼é”™è¯¯çš„Output===\n",
      "{\"year\": 2023, \"month\": å››æœˆ, \"day\": 6, \"era\": \"AD\"}\n",
      "===å‡ºç°å¼‚å¸¸===\n",
      "Failed to parse Date from completion {\"year\": 2023, \"month\": å››æœˆ, \"day\": 6, \"era\": \"AD\"}. Got: Expecting value: line 1 column 25 (char 24)\n",
      "===é‡æ–°è§£æç»“æœ===\n",
      "year=2023 month=4 day=6 era='AD'\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import OutputFixingParser\n",
    "\n",
    "new_parser = OutputFixingParser.from_llm(parser=parser, llm=ChatOpenAI(model=\"gpt-4\"))\n",
    "\n",
    "#æˆ‘ä»¬æŠŠä¹‹å‰outputçš„æ ¼å¼æ”¹é”™\n",
    "output = output.content.replace(\"4\",\"å››æœˆ\")\n",
    "print(\"===æ ¼å¼é”™è¯¯çš„Output===\")\n",
    "print(output)\n",
    "try:\n",
    "    date = parser.parse(output)\n",
    "except Exception as e:\n",
    "    print(\"===å‡ºç°å¼‚å¸¸===\")\n",
    "    print(e)\n",
    "    \n",
    "#ç”¨OutputFixingParserè‡ªåŠ¨ä¿®å¤å¹¶è§£æ\n",
    "date = new_parser.parse(output)\n",
    "print(\"===é‡æ–°è§£æç»“æœ===\")\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>æ€è€ƒï¼š</b>çŒœä¸€ä¸‹OutputFixingParseræ˜¯æ€ä¹ˆåšåˆ°çš„\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4ã€å°ç»“\n",
    "\n",
    "1. LangChain ç»Ÿä¸€å°è£…äº†å„ç§æ¨¡å‹çš„è°ƒç”¨æ¥å£ï¼ŒåŒ…æ‹¬è¡¥å…¨å‹å’Œå¯¹è¯å‹ä¸¤ç§\n",
    "2. LangChain æä¾›äº† PromptTemplate ç±»ï¼Œå¯ä»¥è‡ªå®šä¹‰å¸¦å˜é‡çš„æ¨¡æ¿\n",
    "3. LangChain æä¾›äº†ä¸€äº›åˆ—è¾“å‡ºè§£æå™¨ï¼Œç”¨äºå°†å¤§æ¨¡å‹çš„è¾“å‡ºè§£ææˆç»“æ„åŒ–å¯¹è±¡ï¼›é¢å¤–å¸¦æœ‰è‡ªåŠ¨ä¿®å¤åŠŸèƒ½ã€‚\n",
    "4. ä¸Šè¿°æ¨¡å‹å±äº LangChain ä¸­è¾ƒä¸ºä¼˜ç§€çš„éƒ¨åˆ†ï¼›ç¾ä¸­ä¸è¶³çš„æ˜¯ OutputParser è‡ªèº«çš„ Prompt ç»´æŠ¤åœ¨ä»£ç ä¸­ï¼Œè€¦åˆåº¦è¾ƒé«˜ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## äºŒã€æ•°æ®è¿æ¥å°è£…\n",
    "\n",
    "<img src=\"data_connection.jpg\" style=\"margin-left: 0px\" width=500px>\n",
    "\n",
    "### 2.1 æ–‡æ¡£åŠ è½½å™¨ï¼šDocument Loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pypdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 : Open Foundation and Fine-Tuned Chat Models\n",
      "Hugo Touvronâˆ—Louis Martinâ€ Kevin Stoneâ€ \n",
      "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
      "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
      "Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\n",
      "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n",
      "Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\n",
      "Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\n",
      "Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\n",
      "Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\n",
      "Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
      "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
      "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n",
      "Sergey Edunov Thomas Scialomâˆ—\n",
      "GenAI, Meta\n",
      "Abstract\n",
      "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "Our fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our\n",
      "models outperform open-source chat models on most benchmarks we tested, and based on\n",
      "ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed-\n",
      "source models. We provide a detailed description of our approach to fine-tuning and safety\n",
      "improvements of Llama 2-Chat in order to enable the community to build on our work and\n",
      "contribute to the responsible development of LLMs.\n",
      "âˆ—Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\n",
      "â€ Second author\n",
      "Contributions for all the authors can be found in Section A.1.arXiv:2307.09288v2  [cs.CL]  19 Jul 2023\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"llama2.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 æ–‡æ¡£å¤„ç†å™¨\n",
    "\n",
    "### 2.2.1 TextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 : Open Foundation and Fine-Tuned Chat Models\n",
      "Hugo Touvronâˆ—Louis Martinâ€ Kevin Stoneâ€ \n",
      "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
      "-------\n",
      "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
      "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
      "-------\n",
      "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
      "Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\n",
      "-------\n",
      "Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\n",
      "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n",
      "-------\n",
      "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n",
      "Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\n",
      "-------\n",
      "Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\n",
      "Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\n",
      "-------\n",
      "Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\n",
      "Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\n",
      "-------\n",
      "Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\n",
      "Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\n",
      "-------\n",
      "Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\n",
      "Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
      "-------\n",
      "Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
      "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
      "-------\n",
      "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
      "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n",
      "Sergey Edunov Thomas Scialomâˆ—\n",
      "-------\n",
      "Sergey Edunov Thomas Scialomâˆ—\n",
      "GenAI, Meta\n",
      "Abstract\n",
      "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\n",
      "-------\n",
      "Abstract\n",
      "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "-------\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "Our fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our\n",
      "-------\n",
      "Our fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our\n",
      "models outperform open-source chat models on most benchmarks we tested, and based on\n",
      "-------\n",
      "models outperform open-source chat models on most benchmarks we tested, and based on\n",
      "ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed-\n",
      "-------\n",
      "ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed-\n",
      "source models. We provide a detailed description of our approach to fine-tuning and safety\n",
      "-------\n",
      "source models. We provide a detailed description of our approach to fine-tuning and safety\n",
      "improvements of Llama 2-Chat in order to enable the community to build on our work and\n",
      "-------\n",
      "improvements of Llama 2-Chat in order to enable the community to build on our work and\n",
      "contribute to the responsible development of LLMs.\n",
      "-------\n",
      "contribute to the responsible development of LLMs.\n",
      "âˆ—Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\n",
      "â€ Second author\n",
      "-------\n",
      "âˆ—Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\n",
      "â€ Second author\n",
      "Contributions for all the authors can be found in Section A.1.arXiv:2307.09288v2  [cs.CL]  19 Jul 2023\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=100,  # æ€è€ƒï¼šä¸ºä»€ä¹ˆè¦åšoverlap\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "paragraphs = text_splitter.create_documents([pages[0].page_content])\n",
    "for para in paragraphs:\n",
    "    print(para.page_content)\n",
    "    print('-------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "LangChain çš„ PDFLoader å’Œ TextSplitter å®ç°éƒ½æ¯”è¾ƒç²—ç³™ï¼Œå®é™…ç”Ÿäº§ä¸­ä¸å»ºè®®ä½¿ç”¨ã€‚\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3ã€å†…ç½®çš„ RAG å®ç° "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2æœ‰7Bã€13Bå’Œ70Bå‚æ•°çš„å˜ä½“ã€‚\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# åŠ è½½æ–‡æ¡£\n",
    "loader = PyPDFLoader(\"llama2.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# æ–‡æ¡£åˆ‡åˆ†\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([pages[2].page_content,pages[3].page_content])\n",
    "\n",
    "# çŒåº“\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "# LangChainå†…ç½®çš„ RAG å®ç°\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(temperature=0), \n",
    "    retriever=db.as_retriever() \n",
    ")\n",
    "\n",
    "query = \"llama 2æœ‰å¤šå°‘å‚æ•°ï¼Ÿ\"\n",
    "response = qa_chain.invoke(query)\n",
    "print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4ã€å°ç»“\n",
    "\n",
    "1. è¿™éƒ¨åˆ†èƒ½åŠ› LangChain çš„å®ç°éå¸¸ç²—ç³™ï¼›\n",
    "2. å®é™…ç”Ÿäº§ä¸­ï¼Œå»ºè®®è‡ªå·±å®ç°ï¼Œä¸å»ºè®®ç”¨ LangChain çš„å·¥å…·ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸‰ã€è®°å¿†å°è£…ï¼šMemory\n",
    "\n",
    "### 3.1ã€å¯¹è¯ä¸Šä¸‹æ–‡ï¼šConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: ä½ å¥½å•Š\\nAI: ä½ ä¹Ÿå¥½å•Š'}\n",
      "{'history': 'Human: ä½ å¥½å•Š\\nAI: ä½ ä¹Ÿå¥½å•Š\\nHuman: ä½ å†å¥½å•Š\\nAI: ä½ åˆå¥½å•Š'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
    "\n",
    "history = ConversationBufferMemory()\n",
    "history.save_context({\"input\": \"ä½ å¥½å•Š\"}, {\"output\": \"ä½ ä¹Ÿå¥½å•Š\"})\n",
    "\n",
    "print(history.load_memory_variables({}))\n",
    "\n",
    "history.save_context({\"input\": \"ä½ å†å¥½å•Š\"}, {\"output\": \"ä½ åˆå¥½å•Š\"})\n",
    "\n",
    "print(history.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2ã€åªä¿ç•™ä¸€ä¸ªçª—å£çš„ä¸Šä¸‹æ–‡ï¼šConversationBufferWindowMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: ç¬¬ä¸‰è½®é—®\\nAI: ç¬¬ä¸‰è½®ç­”'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "window = ConversationBufferWindowMemory(k=1)\n",
    "window.save_context({\"input\": \"ç¬¬ä¸€è½®é—®\"}, {\"output\": \"ç¬¬ä¸€è½®ç­”\"})\n",
    "window.save_context({\"input\": \"ç¬¬äºŒè½®é—®\"}, {\"output\": \"ç¬¬äºŒè½®ç­”\"})\n",
    "window.save_context({\"input\": \"ç¬¬ä¸‰è½®é—®\"}, {\"output\": \"ç¬¬ä¸‰è½®ç­”\"})\n",
    "print(window.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3ã€é€šè¿‡ Token æ•°æ§åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦ï¼šConversationTokenBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: ä½ ä¼šå¹²ä»€ä¹ˆ\\nAI: æˆ‘ä»€ä¹ˆéƒ½ä¼š'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "memory = ConversationTokenBufferMemory(\n",
    "    llm=ChatOpenAI(),\n",
    "    max_token_limit=40\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"ä½ å¥½å•Š\"}, {\"output\": \"ä½ å¥½ï¼Œæˆ‘æ˜¯ä½ çš„AIåŠ©æ‰‹ã€‚\"})\n",
    "memory.save_context(\n",
    "    {\"input\": \"ä½ ä¼šå¹²ä»€ä¹ˆ\"}, {\"output\": \"æˆ‘ä»€ä¹ˆéƒ½ä¼š\"})\n",
    "\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4ã€æ›´å¤šç±»å‹\n",
    "\n",
    "- ConversationSummaryMemory: å¯¹ä¸Šä¸‹æ–‡åšæ‘˜è¦\n",
    "  - https://python.langchain.com/docs/modules/memory/types/summary\n",
    "- ConversationSummaryBufferMemory: ä¿å­˜ Token æ•°é™åˆ¶å†…çš„ä¸Šä¸‹æ–‡ï¼Œå¯¹æ›´æ—©çš„åšæ‘˜è¦\n",
    "  - https://python.langchain.com/docs/modules/memory/types/summary_buffer\n",
    "- VectorStoreRetrieverMemory: å°† Memory å­˜å‚¨åœ¨å‘é‡æ•°æ®åº“ä¸­ï¼Œæ ¹æ®ç”¨æˆ·è¾“å…¥æ£€ç´¢å›æœ€ç›¸å…³çš„éƒ¨åˆ†\n",
    "  - https://python.langchain.com/docs/modules/memory/types/vectorstore_retriever_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5ã€å°ç»“\n",
    "\n",
    "1. LangChain çš„ Memory ç®¡ç†æœºåˆ¶å±äºå¯ç”¨çš„éƒ¨åˆ†ï¼Œå°¤å…¶æ˜¯ç®€å•æƒ…å†µå¦‚æŒ‰è½®æ•°æˆ–æŒ‰ Token æ•°ç®¡ç†ï¼›\n",
    "2. å¯¹äºå¤æ‚æƒ…å†µï¼Œå®ƒä¸ä¸€å®šæ˜¯æœ€ä¼˜çš„å®ç°ï¼Œä¾‹å¦‚æ£€ç´¢å‘é‡åº“æ–¹å¼ï¼Œå»ºè®®æ ¹æ®å®é™…æƒ…å†µå’Œæ•ˆæœè¯„ä¼°ï¼›\n",
    "3. ä½†æ˜¯**å®ƒå¯¹å†…å­˜çš„å„ç§ç»´æŠ¤æ–¹æ³•çš„æ€è·¯åœ¨å®é™…ç”Ÿäº§ä¸­å¯ä»¥å€Ÿé‰´**ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å››ã€Chain å’Œ LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain Expression Languageï¼ˆLCELï¼‰æ˜¯ä¸€ç§å£°æ˜å¼è¯­è¨€ï¼Œå¯è½»æ¾ç»„åˆä¸åŒçš„è°ƒç”¨é¡ºåºæ„æˆ Chainã€‚LCEL è‡ªåˆ›ç«‹ä¹‹åˆå°±è¢«è®¾è®¡ä¸ºèƒ½å¤Ÿæ”¯æŒå°†åŸå‹æŠ•å…¥ç”Ÿäº§ç¯å¢ƒï¼Œ**æ— éœ€ä»£ç æ›´æ”¹**ï¼Œä»æœ€ç®€å•çš„â€œæç¤º+LLMâ€é“¾åˆ°æœ€å¤æ‚çš„é“¾ï¼ˆå·²æœ‰ç”¨æˆ·æˆåŠŸåœ¨ç”Ÿäº§ç¯å¢ƒä¸­è¿è¡ŒåŒ…å«æ•°ç™¾ä¸ªæ­¥éª¤çš„ LCEL Chainï¼‰ã€‚\n",
    "\n",
    "LCELçš„ä¸€äº›äº®ç‚¹åŒ…æ‹¬ï¼š\n",
    "\n",
    "1. **æµæ”¯æŒ**ï¼šä½¿ç”¨ LCEL æ„å»º Chain æ—¶ï¼Œä½ å¯ä»¥è·å¾—æœ€ä½³çš„é¦–ä¸ªä»¤ç‰Œæ—¶é—´ï¼ˆå³ä»è¾“å‡ºå¼€å§‹åˆ°é¦–æ‰¹è¾“å‡ºç”Ÿæˆçš„æ—¶é—´ï¼‰ã€‚å¯¹äºæŸäº› Chainï¼Œè¿™æ„å‘³ç€å¯ä»¥ç›´æ¥ä»LLMæµå¼ä¼ è¾“ä»¤ç‰Œåˆ°æµè¾“å‡ºè§£æå™¨ï¼Œä»è€Œä»¥ä¸ LLM æä¾›å•†è¾“å‡ºåŸå§‹ä»¤ç‰Œç›¸åŒçš„é€Ÿç‡è·å¾—è§£æåçš„ã€å¢é‡çš„è¾“å‡ºã€‚\n",
    "\n",
    "2. **å¼‚æ­¥æ”¯æŒ**ï¼šä»»ä½•ä½¿ç”¨ LCEL æ„å»ºçš„é“¾æ¡éƒ½å¯ä»¥é€šè¿‡åŒæ­¥APIï¼ˆä¾‹å¦‚ï¼Œåœ¨ Jupyter ç¬”è®°æœ¬ä¸­è¿›è¡ŒåŸå‹è®¾è®¡æ—¶ï¼‰å’Œå¼‚æ­¥ APIï¼ˆä¾‹å¦‚ï¼Œåœ¨ LangServe æœåŠ¡å™¨ä¸­ï¼‰è°ƒç”¨ã€‚è¿™ä½¿å¾—ç›¸åŒçš„ä»£ç å¯ç”¨äºåŸå‹è®¾è®¡å’Œç”Ÿäº§ç¯å¢ƒï¼Œå…·æœ‰å‡ºè‰²çš„æ€§èƒ½ï¼Œå¹¶èƒ½å¤Ÿåœ¨åŒä¸€æœåŠ¡å™¨ä¸­å¤„ç†å¤šä¸ªå¹¶å‘è¯·æ±‚ã€‚\n",
    "\n",
    "3. **ä¼˜åŒ–çš„å¹¶è¡Œæ‰§è¡Œ**ï¼šå½“ä½ çš„ LCEL é“¾æ¡æœ‰å¯ä»¥å¹¶è¡Œæ‰§è¡Œçš„æ­¥éª¤æ—¶ï¼ˆä¾‹å¦‚ï¼Œä»å¤šä¸ªæ£€ç´¢å™¨ä¸­è·å–æ–‡æ¡£ï¼‰ï¼Œæˆ‘ä»¬ä¼šè‡ªåŠ¨æ‰§è¡Œï¼Œæ— è®ºæ˜¯åœ¨åŒæ­¥è¿˜æ˜¯å¼‚æ­¥æ¥å£ä¸­ï¼Œä»¥å®ç°æœ€å°çš„å»¶è¿Ÿã€‚\n",
    "\n",
    "4. **é‡è¯•å’Œå›é€€**ï¼šä¸º LCEL é“¾çš„ä»»ä½•éƒ¨åˆ†é…ç½®é‡è¯•å’Œå›é€€ã€‚è¿™æ˜¯ä½¿é“¾åœ¨è§„æ¨¡ä¸Šæ›´å¯é çš„ç»ä½³æ–¹å¼ã€‚ç›®å‰æˆ‘ä»¬æ­£åœ¨æ·»åŠ é‡è¯•/å›é€€çš„æµåª’ä½“æ”¯æŒï¼Œå› æ­¤ä½ å¯ä»¥åœ¨ä¸å¢åŠ ä»»ä½•å»¶è¿Ÿæˆæœ¬çš„æƒ…å†µä¸‹è·å¾—å¢åŠ çš„å¯é æ€§ã€‚\n",
    "\n",
    "5. **è®¿é—®ä¸­é—´ç»“æœ**ï¼šå¯¹äºæ›´å¤æ‚çš„é“¾æ¡ï¼Œè®¿é—®åœ¨æœ€ç»ˆè¾“å‡ºäº§ç”Ÿä¹‹å‰çš„ä¸­é—´æ­¥éª¤çš„ç»“æœé€šå¸¸éå¸¸æœ‰ç”¨ã€‚è¿™å¯ä»¥ç”¨äºè®©æœ€ç»ˆç”¨æˆ·çŸ¥é“æ­£åœ¨å‘ç”Ÿä¸€äº›äº‹æƒ…ï¼Œç”šè‡³ä»…ç”¨äºè°ƒè¯•é“¾æ¡ã€‚ä½ å¯ä»¥æµå¼ä¼ è¾“ä¸­é—´ç»“æœï¼Œå¹¶ä¸”åœ¨æ¯ä¸ªLangServeæœåŠ¡å™¨ä¸Šéƒ½å¯ç”¨ã€‚\n",
    "\n",
    "6. **è¾“å…¥å’Œè¾“å‡ºæ¨¡å¼**ï¼šè¾“å…¥å’Œè¾“å‡ºæ¨¡å¼ä¸ºæ¯ä¸ª LCEL é“¾æä¾›äº†ä»é“¾çš„ç»“æ„æ¨æ–­å‡ºçš„ Pydantic å’Œ JSONSchema æ¨¡å¼ã€‚è¿™å¯ä»¥ç”¨äºè¾“å…¥å’Œè¾“å‡ºçš„éªŒè¯ï¼Œæ˜¯ LangServe çš„ä¸€ä¸ªç»„æˆéƒ¨åˆ†ã€‚\n",
    "\n",
    "7. **æ— ç¼LangSmithè·Ÿè¸ªé›†æˆ**ï¼šéšç€é“¾æ¡å˜å¾—è¶Šæ¥è¶Šå¤æ‚ï¼Œç†è§£æ¯ä¸€æ­¥å‘ç”Ÿäº†ä»€ä¹ˆå˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚é€šè¿‡ LCELï¼Œæ‰€æœ‰æ­¥éª¤éƒ½è‡ªåŠ¨è®°å½•åˆ° LangSmithï¼Œä»¥å®ç°æœ€å¤§çš„å¯è§‚å¯Ÿæ€§å’Œå¯è°ƒè¯•æ€§ã€‚\n",
    "\n",
    "8. **æ— ç¼LangServeéƒ¨ç½²é›†æˆ**ï¼šä»»ä½•ä½¿ç”¨ LCEL åˆ›å»ºçš„é“¾éƒ½å¯ä»¥è½»æ¾åœ°ä½¿ç”¨ LangServe è¿›è¡Œéƒ¨ç½²ã€‚\n",
    "\n",
    "åŸæ–‡ï¼šhttps://python.langchain.com/docs/expression_language/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### çœ‹ä¸ªä¾‹å­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List, Dict, Optional\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name=None price_lower=None price_upper=100 data_lower=None data_upper=None sort_by=<SortEnum.data: 'data'> ordering=<OrderingEnum.descend: 'descend'>\n"
     ]
    }
   ],
   "source": [
    "# è¾“å‡ºç»“æ„\n",
    "class SortEnum(str, Enum):\n",
    "    data = 'data'\n",
    "    price = 'price'\n",
    "\n",
    "class OrderingEnum(str, Enum):\n",
    "    ascend = 'ascend'\n",
    "    descend = 'descend'\n",
    "\n",
    "class Semantics(BaseModel):\n",
    "    name: Optional[str] = Field(description=\"æµé‡åŒ…åç§°\",default=None)\n",
    "    price_lower: Optional[int] = Field(description=\"ä»·æ ¼ä¸‹é™\",default=None)\n",
    "    price_upper: Optional[int] = Field(description=\"ä»·æ ¼ä¸Šé™\",default=None)\n",
    "    data_lower: Optional[int] = Field(description=\"æµé‡ä¸‹é™\",default=None)\n",
    "    data_upper: Optional[int] = Field(description=\"æµé‡ä¸Šé™\",default=None)\n",
    "    sort_by: Optional[SortEnum] = Field(description=\"æŒ‰ä»·æ ¼æˆ–æµé‡æ’åº\",default=None)\n",
    "    ordering: Optional[OrderingEnum] = Field(description=\"å‡åºæˆ–é™åºæ’åˆ—\",default=None)\n",
    "\n",
    "# OutputParser\n",
    "parser = PydanticOutputParser(pydantic_object=Semantics)\n",
    "\n",
    "# Prompt æ¨¡æ¿\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"å°†ç”¨æˆ·çš„è¾“å…¥è§£ææˆJSONè¡¨ç¤ºã€‚è¾“å‡ºæ ¼å¼å¦‚ä¸‹ï¼š\\n{format_instructions}\\nä¸è¦è¾“å‡ºæœªæåŠçš„å­—æ®µã€‚\",\n",
    "        ),\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    ").partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "# æ¨¡å‹\n",
    "model = ChatOpenAI(temperature=0)\n",
    "\n",
    "# LCEL è¡¨è¾¾å¼\n",
    "runnable = (\n",
    "    {\"query\": RunnablePassthrough()} | prompt | model | parser\n",
    ")\n",
    "\n",
    "# è¿è¡Œ\n",
    "print(runnable.invoke(\"ä¸è¶…è¿‡100å…ƒçš„æµé‡å¤§çš„å¥—é¤æœ‰å“ªäº›\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ¢ä¸ªå¤æ‚ä¸€ç‚¹çš„ \n",
    "\n",
    "å›å¿† SK ä¸­çš„åµŒå¥—è°ƒç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OpenAIçš„CEOæ˜¯Sam Altmanã€‚'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# å‘é‡æ•°æ®åº“\n",
    "vectorstore = Chroma.from_texts(\n",
    "    [\n",
    "        \"Sam Altmanæ˜¯OpenAIçš„CEO\", \n",
    "        \"Sam Altmanè¢«è§£é›‡äº†\",\n",
    "        \"Sam Altmanè¢«å¤èŒäº†\"\n",
    "    ], embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# æ£€ç´¢æ¥å£\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Promptæ¨¡æ¿\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Chain\n",
    "retrieval_chain = (\n",
    "    {\"question\": RunnablePassthrough(),\"context\": retriever}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "retrieval_chain.invoke(\"OpenAIçš„CEOæ˜¯è°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**æ³¨æ„**: åœ¨å½“å‰çš„æ–‡æ¡£ä¸­ LCEL äº§ç”Ÿçš„å¯¹è±¡ï¼Œè¢«å«åš runnable æˆ– chainï¼Œç»å¸¸ä¸¤ç§å«æ³•æ··ç”¨ã€‚æœ¬è´¨å°±æ˜¯ä¸€ä¸ªè‡ªå®šä¹‰è°ƒç”¨æµç¨‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>ä½¿ç”¨ LCEL çš„ä»·å€¼ï¼Œä¹Ÿå°±æ˜¯ LangChain çš„æ ¸å¿ƒä»·å€¼ã€‚</b> <br />\n",
    "å®˜æ–¹ä»ä¸åŒè§’åº¦ç»™å‡ºäº†ä¸¾ä¾‹è¯´æ˜ï¼šhttps://python.langchain.com/docs/expression_language/why\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é€šè¿‡ LCELï¼Œè¿˜å¯ä»¥å®ç°\n",
    "\n",
    "1. é…ç½®è¿è¡Œæ—¶å˜é‡ï¼šhttps://python.langchain.com/docs/expression_language/how_to/configure\n",
    "2. æ•…éšœå›é€€ï¼šhttps://python.langchain.com/docs/expression_language/how_to/fallbacks\n",
    "3. å¹¶è¡Œè°ƒç”¨ï¼šhttps://python.langchain.com/docs/expression_language/how_to/map\n",
    "4. é€»è¾‘åˆ†æ”¯ï¼šhttps://python.langchain.com/docs/expression_language/how_to/routing\n",
    "5. è°ƒç”¨è‡ªå®šä¹‰æµå¼å‡½æ•°ï¼šhttps://python.langchain.com/docs/expression_language/how_to/generators\n",
    "6. é“¾æ¥å¤–éƒ¨Memoryï¼šhttps://python.langchain.com/docs/expression_language/how_to/message_history\n",
    "\n",
    "æ›´å¤šä¾‹å­ï¼šhttps://python.langchain.com/docs/expression_language/cookbook/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>æ€è€ƒï¼š</b>ä»æ¨¡å—é—´è§£ä¾èµ–è§’åº¦ï¼ŒLCELçš„æ„ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## äº”ã€æ™ºèƒ½ä½“æ¶æ„ï¼šAgent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 å›å¿†ï¼šä»€ä¹ˆæ˜¯æ™ºèƒ½ä½“ï¼ˆAgentï¼‰\n",
    "\n",
    "å°†å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºä¸€ä¸ªæ¨ç†å¼•æ“ã€‚ç»™å®šä¸€ä¸ªä»»åŠ¡ï¼Œæ™ºèƒ½ä½“è‡ªåŠ¨ç”Ÿæˆå®Œæˆä»»åŠ¡æ‰€éœ€çš„æ­¥éª¤ï¼Œæ‰§è¡Œç›¸åº”åŠ¨ä½œï¼ˆä¾‹å¦‚é€‰æ‹©å¹¶è°ƒç”¨å·¥å…·ï¼‰ï¼Œç›´åˆ°ä»»åŠ¡å®Œæˆã€‚\n",
    "\n",
    "<img src=\"agent-overview.png\" style=\"margin-left: 0px\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 å…ˆå®šä¹‰ä¸€äº›å·¥å…·ï¼šTools\n",
    "\n",
    "- å¯ä»¥æ˜¯ä¸€ä¸ªå‡½æ•°æˆ–ä¸‰æ–¹ API\n",
    "- ä¹Ÿå¯ä»¥æŠŠä¸€ä¸ª Chain æˆ–è€… Agent çš„ run()ä½œä¸ºä¸€ä¸ª Tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import SerpAPIWrapper\n",
    "from langchain.tools import Tool, tool\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "tools = [\n",
    "    Tool.from_function(\n",
    "        func=search.run,\n",
    "        name=\"Search\",\n",
    "        description=\"useful for when you need to answer questions about current events\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "import dateutil.parser as parser\n",
    "from datetime import date\n",
    "\n",
    "# è‡ªå®šä¹‰å·¥å…·\n",
    "@tool(\"weekday\")\n",
    "def weekday(date_str: str) -> str:\n",
    "    \"\"\"Convert date to weekday name\"\"\"\n",
    "    d = parser.parse(date_str)\n",
    "    return calendar.day_name[d.weekday()]\n",
    "\n",
    "tools += [weekday]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 æ™ºèƒ½ä½“ç±»å‹ï¼šReAct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ReAct.png\" style=\"margin-left: 0px\" width=500px>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-search-results\n",
    "!pip install langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "{tools}\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [{tool_names}]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "import json\n",
    "\n",
    "# ä¸‹è½½ä¸€ä¸ªç°æœ‰çš„ Prompt æ¨¡æ¿\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "print(prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mæˆ‘éœ€è¦çŸ¥é“å‘¨æ°ä¼¦çš„ç”Ÿæ—¥æ˜¯å“ªä¸€å¤©ï¼Œç„¶åæˆ‘å¯ä»¥ä½¿ç”¨weekdayå‡½æ•°æ¥æ‰¾å‡ºé‚£å¤©æ˜¯æ˜ŸæœŸå‡ ã€‚\n",
      "Action: Search\n",
      "Action Input: å‘¨æ°ä¼¦çš„ç”Ÿæ—¥\u001b[0m\u001b[36;1m\u001b[1;3mJanuary 18, 1979\u001b[0m\u001b[32;1m\u001b[1;3mæˆ‘ç°åœ¨çŸ¥é“å‘¨æ°ä¼¦çš„ç”Ÿæ—¥æ˜¯1æœˆ18æ—¥ï¼Œæˆ‘å¯ä»¥ä½¿ç”¨weekdayå‡½æ•°æ¥æ‰¾å‡ºé‚£å¤©æ˜¯æ˜ŸæœŸå‡ ã€‚\n",
      "Action: weekday\n",
      "Action Input: \"1979-01-18\"\u001b[0m\u001b[33;1m\u001b[1;3mThursday\u001b[0m\u001b[32;1m\u001b[1;3mæˆ‘ç°åœ¨çŸ¥é“å‘¨æ°ä¼¦çš„ç”Ÿæ—¥é‚£å¤©æ˜¯æ˜ŸæœŸå››ã€‚\n",
      "Final Answer: æ˜ŸæœŸå››\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'å‘¨æ°ä¼¦ç”Ÿæ—¥é‚£å¤©æ˜¯æ˜ŸæœŸå‡ ', 'output': 'æ˜ŸæœŸå››'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "\n",
    "# å®šä¹‰ä¸€ä¸ª agent: éœ€è¦å¤§æ¨¡å‹ã€å·¥å…·é›†ã€å’Œ Prompt æ¨¡æ¿\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "# å®šä¹‰ä¸€ä¸ªæ‰§è¡Œå™¨ï¼šéœ€è¦ agent å¯¹è±¡ å’Œ å·¥å…·é›†\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# æ‰§è¡Œ\n",
    "agent_executor.invoke({\"input\": \"å‘¨æ°ä¼¦ç”Ÿæ—¥é‚£å¤©æ˜¯æ˜ŸæœŸå‡ \"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 æ™ºèƒ½ä½“ç±»å‹ï¼šSelfAskWithSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who lived longer, Muhammad Ali or Alan Turing?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: How old was Muhammad Ali when he died?\n",
      "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
      "Follow up: How old was Alan Turing when he died?\n",
      "Intermediate answer: Alan Turing was 41 years old when he died.\n",
      "So the final answer is: Muhammad Ali\n",
      "\n",
      "Question: When was the founder of craigslist born?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who was the founder of craigslist?\n",
      "Intermediate answer: Craigslist was founded by Craig Newmark.\n",
      "Follow up: When was Craig Newmark born?\n",
      "Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
      "So the final answer is: December 6, 1952\n",
      "\n",
      "Question: Who was the maternal grandfather of George Washington?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who was the mother of George Washington?\n",
      "Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
      "Follow up: Who was the father of Mary Ball Washington?\n",
      "Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
      "So the final answer is: Joseph Ball\n",
      "\n",
      "Question: Are both the directors of Jaws and Casino Royale from the same country?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who is the director of Jaws?\n",
      "Intermediate answer: The director of Jaws is Steven Spielberg.\n",
      "Follow up: Where is Steven Spielberg from?\n",
      "Intermediate answer: The United States.\n",
      "Follow up: Who is the director of Casino Royale?\n",
      "Intermediate answer: The director of Casino Royale is Martin Campbell.\n",
      "Follow up: Where is Martin Campbell from?\n",
      "Intermediate answer: New Zealand.\n",
      "So the final answer is: No\n",
      "\n",
      "Question: {input}\n",
      "Are followup questions needed here:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "# ä¸‹è½½ä¸€ä¸ªæ¨¡æ¿\n",
    "prompt = hub.pull(\"hwchase17/self-ask-with-search\")\n",
    "\n",
    "print(prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mYes.\n",
      "Follow up: Who is å´äº¬'s wife?\u001b[0m\u001b[36;1m\u001b[1;3m['ç®€ä»‹ï¼š ä½ çŸ¥é“å´äº¬å¨¶è¿‡å‡ ä¸ªè€å©†å—åªç»“äº†ä¸€æ¬¡å©šç›®å‰ä»–æœ‰ä¸¤ä¸ªå­©å­å´äº¬çš„è€å©†æ˜¯è°¢æ¥ ä»–ä»¬æ˜¯åœ¨2014å¹´ç»“çš„å©š2018å¹´ç”Ÿçš„ç¬¬äºŒä¸ªå­©å­å´è™‘1974å¹´4... å°æå­çœŸå®å½±åƒ.', \"Li Bingbing's first-time in an English-language film is Wayne Wang's Snow Flower ... ^ ç¦å¸ƒæ–¯ä¸­å›½å‘å¸ƒ100åäººæ¦œ å´äº¬é»„æ¸¤èƒ¡æ­Œä½åˆ—å‰ä¸‰ . Sina Entertainment (in ...\", 'å´äº¬å¾ˆéœ¸æ°”çš„æ‹’ç»äº†å¥¹çš„è¦æ±‚ï¼Œè¡¨ç¤ºä»–åªé€‰å¯¹çš„äººï¼Œè€Œä¸é€‰è´µçš„äººã€‚çœ‹åˆ°è¿™é‡Œï¼Œå¾ˆå¤šç½‘å‹å°±è¦é«˜æ½®äº†ï¼Œâ€œå“¼ï¼Œè¿™äº›æµé‡æ˜æ˜Ÿæ‹½ä»€ä¹ˆæ‹½ï¼Œç°åœ¨è‚ å­éƒ½æ‚”é’äº†å§ï¼â€.', 'Comments Â· é«˜æ¸…å¤§Sï¼Œå´äº¬ï¼Œè‚è¿œç‰ˆï¼ˆç¬¬34é›†ï¼‰ Â· å€©å¥³å¹½é­‚å´äº¬ç‰ˆå¹•åèŠ±çµ® Â· Ending Chapter! Â· CEO attended party with mistress in high-profile, wife ...', 'æ˜æ˜Ÿè®¿è°ˆä¸€æ¡£å®£æ‰¬æ€åº¦çš„æ˜æ˜Ÿè®¿è°ˆæ–°ç»¼è‰ºï¼Œäº¦åŠ¨äº¦é™çš„å¯¹å˜‰å®¾åŠ±å¿—æ•…äº‹åšæ·±åº¦å‰–æï¼Œå…¨æ–¹ä½ã€çœŸå®ã€ç«‹ä½“åœ°å±•ç°å˜‰å®¾çš„å½¢è±¡ã€æ€§æ ¼ï¼Œå±•ç°å˜‰å®¾é²œæ´»çœŸå®çš„ä¸€é¢ã€‚', 'Lixiaopeng&#39;s wife, Zhou Yangqing, is a retired Olympic champion gymnast from China. She won a gold medal in the uneven bars event at the ...', 'ã€Šå½±è§†é£äº‘ã€‹æ ç›®æ˜¯åŒ—äº¬ç”µè§†å°å”¯ä¸€ä¸€æ¡£å¤§å‹å½±è§†è®¿è°ˆèŠ‚ç›®ã€‚ä»¥å›é¡¾ç»å…¸ä¼˜ç§€å½±è§†ä½œå“ã€å®£ä¼ æ¨èå„é¢‘é“çƒ­æ’­ç”µè§†å‰§åŠè¿½è¸ªå›½å†…å³å°†ä¸Šæ˜ ç”µå½±ä¸ºä¸»è¦å†…å®¹ï¼Œ ...', \"Esther's wife Â· æå‰ç»™å¥³å„¿åšæ•°æ®å™œ#è™ä¹¦æ¬£#è™ä¹¦æ¬£æ°¸å¤œæ˜Ÿæ²³#è™ä¹¦æ¬£å°. 13.0 ... å´äº¬å‡ ä¸ªå­©å­. 7454. 00:00 Â· å´äº¬å‡ ä¸ªå­©å­ Â· @ ç æ±Ÿè§†é¢‘ Â· å§œå¦ç»“å©šäº†å—.\", 'ã€FULLã€‘å´äº¬è°¢æ¥ å¤«å¦‡ä¹˜åç”œèœœå†’é™©ä¸“è½¦æˆ˜ç‹¼é“æ±‰æŸ”æƒ…å°½æ˜¾åå·®èŒã€ŠçœŸæ˜Ÿè¯å¤§å†’é™©ã€‹ç¬¬12æœŸ20170724[æµ™æ±Ÿå«è§†å®˜æ–¹HD]. 249K views Â· 6 years ago ...more ...']\u001b[0m\u001b[32;1m\u001b[1;3mFollow up: What variety shows has è°¢æ¥  hosted?\u001b[0m\u001b[36;1m\u001b[1;3m['TBA, Love Actually Season 3 add. Chinese TV Show, 0000, 10 eps. (Main Host). 10 ; 2023, Ace vs Ace Season 8 add. Chinese TV Show, 2023, 12 eps. (Ep. 1) (Guest).', 'ç”Ÿå¹³ äº2005å¹´â€œçŒ«äººè¶…çº§é­…åŠ›ä¸»æŒç§€â€å† å†›è„±é¢–è€Œå‡ºï¼Œç°ä»»å…‰çº¿ä¼ åª’æ——ä¸‹ä¸»æ‰“èŠ‚ç›®ã€Šå¨±ä¹ç°åœºã€‹ã€ã€Šæœ€ä½³ç°åœºã€‹ã€ã€Šå½±è§†é£äº‘æ¦œã€‹å½“å®¶ä¸»æŒã€‚ 2011å¹´11æœˆ24æ—¥,è°¢æ¥ å‘è¡Œé¦–å¼ ä¸ªäººepã€Šæœ€å¥½çš„æˆ‘ä»¬ã€‹ã€‚ 2014å¹´ï¼Œå´äº¬å‘å¸ƒæ–°å¹´å¾®åšå…¬å¸ƒå©šè®¯ï¼Œè¡¨ç¤ºå·²ç»ä¸è°¢æ¥ ç»“å©šã€‚', '2016å¹´ï¼Œä¸»æ¼”çš„å¥‡å¹»ç‰‡ã€Šå¤§è¯è¥¿æ¸¸3ã€‹ä¸Šæ˜ ã€‚ 2017å¹´ï¼Œä¸»æ¼”ç”µå½±ã€Šè¿™ä½å£®å£«ã€‹ã€‚ 2019å¹´ï¼Œåœ¨ç¾é£ŸçœŸäººç§€ã€Šç†Ÿæ‚‰çš„å‘³é“ç¬¬å››å­£ã€‹ä¸­æ‹…ä»»ä¸»æŒäººã€‚ 2020å¹´ï¼Œä¸»æŒçš„åœºæ™¯é—¯å…³å¼äººç‰©è®¿è°ˆèŠ‚ç›®ã€Šè¿½æ¢¦äººä¹‹å¼€åˆäººç”Ÿã€‹æ’­å‡ºï¼›åŒå¹´ï¼Œä½œä¸ºå¸¸é©»å˜‰å®¾å‚åŠ å®æ™¯è§‚å¯ŸèŠ‚ç›®ã€Šå¹¸ç¦ä¸‰é‡å¥ç¬¬ä¸‰å­£ã€‹ã€‚', 'Xie Nan (è°¢æ¥ ) was born on November 6, 1983. Xie Nan movies and tv shows: After Love Actually 2022 (China), Snow Day 2022 (China).', '... has been held for thousands of years. Onentert New 700 views Â· 5:38 Â· Go ... Welcome Back To Sound EP5ã€èŠ’æœTVçˆ±è±†å¨±ä¹ç«™ã€‘. èŠ’æœTVçˆ±è±†MangoTV Idol ...', 'ä¸­å›½å†…åœ°å¥³ä¸»æŒäººã€æ¼”å‘˜.', 'The sixth season of the Chinese reality talent show Sing! China premiered on 30 July 2021, on Zhejiang Television. Li Ronghao returned as coach for his ...', 'to perform. It has then transformed to an election show to choose the current show hosts. The current show model use Interviews and games ...', 'The couple joined the cast of Chinese variety show, â€œHappiness Trio 3â€ (lit. å¹¸ç¦ä¸‰é‡å¥3), as one of three married couples revealing their ...', '... TV is an entertainment reality show aired since July 1997. The show often invites grassroots including kids with talent to perform. It has ...']\u001b[0m\u001b[32;1m\u001b[1;3mSo the final answer is: è°¢æ¥  has hosted shows like \"å¨±ä¹ç°åœº\", \"æœ€ä½³ç°åœº\", \"å½±è§†é£äº‘æ¦œ\", \"ç†Ÿæ‚‰çš„å‘³é“ç¬¬å››å­£\", and \"è¿½æ¢¦äººä¹‹å¼€åˆäººç”Ÿ\".\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'å´äº¬çš„è€å©†ä¸»æŒè¿‡å“ªäº›ç»¼è‰ºèŠ‚ç›®',\n",
       " 'output': 'è°¢æ¥  has hosted shows like \"å¨±ä¹ç°åœº\", \"æœ€ä½³ç°åœº\", \"å½±è§†é£äº‘æ¦œ\", \"ç†Ÿæ‚‰çš„å‘³é“ç¬¬å››å­£\", and \"è¿½æ¢¦äººä¹‹å¼€åˆäººç”Ÿ\".'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_self_ask_with_search_agent\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Intermediate Answer\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to ask with search.\",\n",
    "    )\n",
    "]\n",
    "\n",
    "# self_ask_with_search_agent åªèƒ½ä¼ ä¸€ä¸ªåä¸º 'Intermediate Answer' çš„ tool\n",
    "agent = create_self_ask_with_search_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "agent_executor.invoke({\"input\": \"å´äº¬çš„è€å©†ä¸»æŒè¿‡å“ªäº›ç»¼è‰ºèŠ‚ç›®\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 OpenAI Assistants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10å‡4çš„å·®çš„2.3æ¬¡æ–¹æ˜¯61.62ã€‚\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents.openai_assistant import OpenAIAssistantRunnable\n",
    "\n",
    "interpreter_assistant = OpenAIAssistantRunnable.create_assistant(\n",
    "    name=\"langchain assistant\",\n",
    "    instructions=\"You are a personal math tutor. Write and run code to answer math questions.\",\n",
    "    tools=[{\"type\": \"code_interpreter\"}],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")\n",
    "output = interpreter_assistant.invoke({\"content\": \"10å‡4çš„å·®çš„2.3æ¬¡æ–¹æ˜¯å¤šå°‘\"})\n",
    "\n",
    "print(output[0].content[0].text.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>åˆ’é‡ç‚¹ï¼š</b>\n",
    "<ol>\n",
    "<li>ReAct æ˜¯æ¯”è¾ƒå¸¸ç”¨çš„ Planner</li>\n",
    "<li>SelfAskWithSearch æ›´é€‚åˆéœ€è¦å±‚å±‚æ¨ç†çš„åœºæ™¯ï¼ˆä¾‹å¦‚çŸ¥è¯†å›¾è°±ï¼‰</li>\n",
    "<li>OpenAI Assistants ä¸æ˜¯ä¸‡èƒ½çš„ï¼ŒLangChain çš„å®˜æ–¹æ–‡æ¡£é‡Œä¹Ÿä¸å¼ºè°ƒè¿°æ¥å£äº†</li>\n",
    "<li>Agentè½åœ°åº”ç”¨éœ€è¦æ›´å¤šç»†èŠ‚ï¼Œåé¢è¯¾ç¨‹ä¸­æˆ‘ä»¬ä¼šä¸“é—¨è®² Agent çš„å®ç°</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å…­ã€LangServe\n",
    "\n",
    "LangServe ç”¨äºå°† Chain æˆ–è€… Runnable éƒ¨ç½²æˆä¸€ä¸ª REST API æœåŠ¡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£… LangServe\n",
    "!pip install \"langserve[all]\"\n",
    "\n",
    "# ä¹Ÿå¯ä»¥åªå®‰è£…ä¸€ç«¯\n",
    "# !pip install \"langserve[client]\"\n",
    "# !pip install \"langserve[server]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1ã€Serverç«¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python\n",
    "from fastapi import FastAPI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langserve import add_routes\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI(\n",
    "  title=\"LangChain Server\",\n",
    "  version=\"1.0\",\n",
    "  description=\"A simple api server using Langchain's Runnable interfaces\",\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_template(\"è®²ä¸€ä¸ªå…³äº{topic}çš„ç¬‘è¯\")\n",
    "add_routes(\n",
    "    app,\n",
    "    prompt | model,\n",
    "    path=\"/joke\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"localhost\", port=9999)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2ã€Clientç«¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import requests\r\n",
    "\r\n",
    "response = requests.post(\r\n",
    "    \"http://localhost:9999/joke/invoke\",\r\n",
    "    json={'input': {'topic': 'å°æ˜'}}\r\n",
    ")\r\n",
    "print(response.json())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸ƒã€LangChain.js\n",
    "\n",
    "Python ç‰ˆ LangChain çš„å§Šå¦¹é¡¹ç›®ï¼Œéƒ½æ˜¯ç”± Harrison Chase ä¸»ç†ã€‚\n",
    "\n",
    "é¡¹ç›®åœ°å€ï¼šhttps://github.com/langchain-ai/langchainjs\n",
    "\n",
    "æ–‡æ¡£åœ°å€ï¼šhttps://js.langchain.com/docs/\n",
    "\n",
    "ç‰¹è‰²ï¼š\n",
    "\n",
    "1. å¯ä»¥å’Œ Python ç‰ˆ LangChain æ— ç¼å¯¹æ¥\n",
    "\n",
    "2. æŠ½è±¡è®¾è®¡å®Œå…¨ç›¸åŒï¼Œæ¦‚å¿µä¸€ä¸€å¯¹åº”\n",
    "\n",
    "3. æ‰€æœ‰å¯¹è±¡åºåˆ—åŒ–åéƒ½èƒ½è·¨è¯­è¨€ä½¿ç”¨ï¼Œä½† API å·®åˆ«æŒºå¤§ï¼Œä¸è¿‡åœ¨åŠªåŠ›å¯¹é½\n",
    "\n",
    "æ”¯æŒç¯å¢ƒï¼š\n",
    "\n",
    "1. Node.js (ESM and CommonJS) - 18.x, 19.x, 20.x\n",
    "2. Cloudflare Workers\n",
    "3. Vercel / Next.js (Browser, Serverless and Edge functions)\n",
    "4. Supabase Edge Functions\n",
    "5. Browser\n",
    "6. Deno\n",
    "\n",
    "å®‰è£…ï¼š\n",
    "```\n",
    "npm install langchain\n",
    "```\n",
    "\n",
    "å½“å‰é‡ç‚¹ï¼š\n",
    "\n",
    "1. è¿½ä¸Š Python ç‰ˆçš„èƒ½åŠ›ï¼ˆç”šè‡³ä¸ºæ­¤åšäº†ä¸€ä¸ªåŸºäº gpt-3.5-turbo çš„ä»£ç ç¿»è¯‘å™¨ï¼‰\n",
    "2. ä¿æŒå…¼å®¹å°½å¯èƒ½å¤šçš„ç¯å¢ƒ\n",
    "3. å¯¹è´¨é‡å…³æ³¨ä¸å¤šï¼Œéšæ—¶é—´è‡ªç„¶èƒ½è§£å†³"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain ä¸ Semantic Kernel å¯¹æ¯”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| åŠŸèƒ½/å·¥å…·           | LangChain                       | Semantic Kernel                  |\n",
    "|-------------------|:---------------------------------:|:----------------------------------:|\n",
    "| ç‰ˆæœ¬å·        |  0.1.0  | python-0.4.4.dev  |\n",
    "| é€‚é…çš„ LLM        | å¤š   | å°‘ + å¤–éƒ¨ç”Ÿæ€   |\n",
    "| Prompt å·¥å…·        | æ”¯æŒ    | æ”¯æŒ     |\n",
    "| Prompt å‡½æ•°åµŒå¥—    | éœ€è¦é€šè¿‡ LCEL | æ”¯æŒ        |\n",
    "| Prompt æ¨¡æ¿åµŒå¥—    | ä¸æ”¯æŒ  | ä¸æ”¯æŒ       |\n",
    "| è¾“å‡ºè§£æå·¥å…·       | æ”¯æŒ  | ä¸æ”¯æŒ  |\n",
    "| ä¸Šä¸‹æ–‡ç®¡ç†å·¥å…·           | æ”¯æŒ | C#ç‰ˆæ”¯æŒï¼ŒPythonç‰ˆå°šæœªæ”¯æŒ  |\n",
    "| å†…ç½®å·¥å…·           | å¤šï¼Œä½†è‰¯è ä¸é½  | å°‘ + å¤–éƒ¨ç”Ÿæ€  |\n",
    "| ä¸‰æ–¹å‘é‡æ•°æ®åº“é€‚é…           | å¤š | å°‘ + å¤–éƒ¨ç”Ÿæ€  |\n",
    "| æœåŠ¡éƒ¨ç½² | LangServe | ä¸ Azure è¡”æ¥æ›´ä¸æ»‘\n",
    "| ç®¡ç†å·¥å…· | LangSmith/LangFuse | Prompt Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ€»ç»“\n",
    "\n",
    "1. LangChain éšç€ç‰ˆæœ¬è¿­ä»£å¯ç”¨æ€§æœ‰æ˜æ˜¾æå‡\n",
    "2. ä½¿ç”¨ LangChain è¦é¿å¼€å­˜åœ¨å¤§é‡ä»£ç å†… Prompt çš„æ¨¡å—\n",
    "3. å®ƒçš„å†…ç½®åŸºç¡€å·¥å…·ï¼Œå»ºè®®å……åˆ†æµ‹è¯•æ•ˆæœåå†å†³å®šæ˜¯å¦ä½¿ç”¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä½œä¸š\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç”¨ LangChain é‡æ„ ChatPDF çš„ä½œä¸š"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
